* 
* ==> Audit <==
* |--------------|----------------------------|----------|------------------------|---------|---------------------|---------------------|
|   Command    |            Args            | Profile  |          User          | Version |     Start Time      |      End Time       |
|--------------|----------------------------|----------|------------------------|---------|---------------------|---------------------|
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 01 Mar 24 10:48 EST | 01 Mar 24 10:48 EST |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 01 Mar 24 10:48 EST | 01 Mar 24 10:48 EST |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 01 Mar 24 10:49 EST | 01 Mar 24 10:49 EST |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 04 Mar 24 14:13 EST | 04 Mar 24 14:13 EST |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 04 Mar 24 14:58 EST | 04 Mar 24 14:58 EST |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 07 Mar 24 17:22 EST | 07 Mar 24 17:22 EST |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 07 Mar 24 17:25 EST | 07 Mar 24 17:25 EST |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 07 Mar 24 17:26 EST | 07 Mar 24 17:26 EST |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 13 Mar 24 17:20 EDT | 13 Mar 24 17:20 EDT |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 13 Mar 24 17:20 EDT | 13 Mar 24 17:20 EDT |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 13 Mar 24 17:20 EDT | 13 Mar 24 17:20 EDT |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 13 Mar 24 17:36 EDT | 13 Mar 24 17:36 EDT |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 18 Mar 24 11:42 EDT | 18 Mar 24 11:42 EDT |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 18 Mar 24 15:44 EDT | 18 Mar 24 15:44 EDT |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 13:23 EDT | 19 Mar 24 13:23 EDT |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 13:23 EDT | 19 Mar 24 13:23 EDT |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:20 EDT | 19 Mar 24 14:20 EDT |
| start        |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:37 EDT | 19 Mar 24 14:40 EDT |
| ip           |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:46 EDT | 19 Mar 24 14:46 EDT |
| ip           |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:47 EDT | 19 Mar 24 14:47 EDT |
| start        |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:49 EDT | 19 Mar 24 14:50 EDT |
| ip           |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:51 EDT | 19 Mar 24 14:51 EDT |
| start        | --driver=virtualbox        | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:53 EDT |                     |
| delete       |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:54 EDT | 19 Mar 24 14:54 EDT |
| start        | --driver=virtualbox        | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:54 EDT |                     |
| ip           |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:57 EDT |                     |
| start        |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:57 EDT |                     |
| ip           |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:58 EDT |                     |
| delete       |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:59 EDT | 19 Mar 24 14:59 EDT |
| start        |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 14:59 EDT | 19 Mar 24 15:00 EDT |
| ip           |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 16:02 EDT | 19 Mar 24 16:02 EDT |
| ip           |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 18:50 EDT | 19 Mar 24 18:50 EDT |
| ssh          | docker@192.168.49.2        | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 18:53 EDT |                     |
| ssh          |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 18:54 EDT |                     |
| ssh          |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 19:11 EDT | 19 Mar 24 19:17 EDT |
| ssh          | 10.244.0.7                 | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 22:23 EDT |                     |
| ssh          |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 22:24 EDT | 19 Mar 24 22:26 EDT |
| ssh          |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 22:37 EDT | 19 Mar 24 22:37 EDT |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 22:46 EDT | 19 Mar 24 22:46 EDT |
| ssh          |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 23:34 EDT |                     |
| ssh          |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 23:39 EDT |                     |
| ip           |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 23:43 EDT | 19 Mar 24 23:43 EDT |
| ip           | addr                       | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 23:52 EDT | 19 Mar 24 23:52 EDT |
| service      | k8s-web-hello-deploy       | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 19 Mar 24 23:57 EDT |                     |
| service      | k8s-web-hello-deploy       | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 20 Mar 24 00:05 EDT |                     |
| ip           |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 20 Mar 24 00:08 EDT | 20 Mar 24 00:08 EDT |
| service      | k8s-web-hello-deploy       | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 20 Mar 24 00:10 EDT |                     |
| start        |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 20 Mar 24 00:12 EDT | 20 Mar 24 00:13 EDT |
| ip           |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 20 Mar 24 00:13 EDT | 20 Mar 24 00:13 EDT |
| service      | k8s-web-hello-deploy       | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 20 Mar 24 00:15 EDT |                     |
| service      | k8s-web-hello-deploy --url | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 20 Mar 24 00:18 EDT |                     |
| update-check |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 21 Mar 24 12:44 EDT | 21 Mar 24 12:44 EDT |
| start        |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 21 Mar 24 12:58 EDT | 21 Mar 24 13:00 EDT |
| service      | k8s-web-hello-deploy       | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 21 Mar 24 13:10 EDT | 21 Mar 24 13:10 EDT |
| service      | k8s-web-hello-deploy       | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 21 Mar 24 13:12 EDT | 21 Mar 24 13:12 EDT |
| service      | k8s-web-hello-deploy       | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 21 Mar 24 13:13 EDT | 21 Mar 24 13:14 EDT |
| service      | k8s-web-hello-deploy       | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 21 Mar 24 13:27 EDT | 21 Mar 24 13:28 EDT |
| service      | k8s-web-hello-deploy       | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 21 Mar 24 13:29 EDT | 21 Mar 24 13:29 EDT |
| dashboard    |                            | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 21 Mar 24 13:33 EDT |                     |
| service      | k8s-web-hello-svc          | minikube | NANA-DEVOPS\Nana Brown | v1.32.0 | 21 Mar 24 16:27 EDT |                     |
|--------------|----------------------------|----------|------------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/03/21 12:58:51
Running on machine: Nana-DevOps
Binary: Built with gc go1.21.3 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0321 12:58:51.128115   12136 out.go:296] Setting OutFile to fd 184 ...
I0321 12:58:51.130647   12136 out.go:348] isatty.IsTerminal(184) = true
I0321 12:58:51.130647   12136 out.go:309] Setting ErrFile to fd 184...
I0321 12:58:51.130647   12136 out.go:348] isatty.IsTerminal(184) = true
I0321 12:58:51.183055   12136 out.go:303] Setting JSON to false
I0321 12:58:51.191689   12136 start.go:128] hostinfo: {"hostname":"Nana-DevOps","uptime":268749,"bootTime":1710771581,"procs":376,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.3296 Build 22631.3296","kernelVersion":"10.0.22631.3296 Build 22631.3296","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"0587d708-12d1-45b6-b550-8460c03b7f46"}
W0321 12:58:51.191689   12136 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0321 12:58:51.196023   12136 out.go:177] üòÑ  minikube v1.32.0 on Microsoft Windows 11 Pro 10.0.22631.3296 Build 22631.3296
I0321 12:58:51.200321   12136 notify.go:220] Checking for updates...
I0321 12:58:51.200321   12136 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0321 12:58:51.203635   12136 driver.go:378] Setting default libvirt URI to qemu:///system
I0321 12:58:51.476406   12136 docker.go:122] docker version: linux-25.0.2:Docker Desktop 4.27.1 (136059)
I0321 12:58:51.490820   12136 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0321 12:58:52.742426   12136 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.2516064s)
I0321 12:58:52.746274   12136 info.go:266] docker info: {ID:386a14c5-ef11-460a-a358-30cbd6e0e1cf Containers:5 ContainersRunning:1 ContainersPaused:0 ContainersStopped:4 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:42 OomKillDisable:true NGoroutines:71 SystemTime:2024-03-21 16:58:52.656757413 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:11 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16614838272 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.3-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.22] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.3.0]] Warnings:<nil>}}
I0321 12:58:52.750330   12136 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0321 12:58:52.752440   12136 start.go:298] selected driver: docker
I0321 12:58:52.752440   12136 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:8100 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Nana Brown:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0321 12:58:52.752714   12136 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0321 12:58:52.831420   12136 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0321 12:58:53.654016   12136 info.go:266] docker info: {ID:386a14c5-ef11-460a-a358-30cbd6e0e1cf Containers:5 ContainersRunning:1 ContainersPaused:0 ContainersStopped:4 Images:7 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:42 OomKillDisable:true NGoroutines:71 SystemTime:2024-03-21 16:58:53.603721423 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:11 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16614838272 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.3-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.22] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.3.0]] Warnings:<nil>}}
I0321 12:58:53.747406   12136 cni.go:84] Creating CNI manager for ""
I0321 12:58:53.748037   12136 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0321 12:58:53.748037   12136 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:8100 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Nana Brown:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0321 12:58:53.751545   12136 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0321 12:58:53.753935   12136 cache.go:121] Beginning downloading kic base image for docker with docker
I0321 12:58:53.755088   12136 out.go:177] üöú  Pulling base image ...
I0321 12:58:53.756839   12136 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0321 12:58:53.757585   12136 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0321 12:58:53.759352   12136 preload.go:148] Found local preload: C:\Users\Nana Brown\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0321 12:58:53.759948   12136 cache.go:56] Caching tarball of preloaded images
I0321 12:58:53.760602   12136 preload.go:174] Found C:\Users\Nana Brown\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0321 12:58:53.760602   12136 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0321 12:58:53.761170   12136 profile.go:148] Saving config to C:\Users\Nana Brown\.minikube\profiles\minikube\config.json ...
I0321 12:58:54.114981   12136 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0321 12:58:54.114981   12136 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0321 12:58:54.115956   12136 cache.go:194] Successfully downloaded all kic artifacts
I0321 12:58:54.117203   12136 start.go:365] acquiring machines lock for minikube: {Name:mkdda2152346298c5c25e519a8827f4e77500de3 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0321 12:58:54.117321   12136 start.go:369] acquired machines lock for "minikube" in 63.7¬µs
I0321 12:58:54.117517   12136 start.go:96] Skipping create...Using existing machine configuration
I0321 12:58:54.117517   12136 fix.go:54] fixHost starting: 
I0321 12:58:54.164538   12136 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0321 12:58:54.578683   12136 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0321 12:58:54.578683   12136 fix.go:128] unexpected machine state, will restart: <nil>
I0321 12:58:54.580292   12136 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0321 12:58:54.595217   12136 cli_runner.go:164] Run: docker start minikube
I0321 12:58:55.538992   12136 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0321 12:58:55.814319   12136 kic.go:430] container "minikube" state is running.
I0321 12:58:55.839932   12136 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0321 12:58:56.122792   12136 profile.go:148] Saving config to C:\Users\Nana Brown\.minikube\profiles\minikube\config.json ...
I0321 12:58:56.125151   12136 machine.go:88] provisioning docker machine ...
I0321 12:58:56.125151   12136 ubuntu.go:169] provisioning hostname "minikube"
I0321 12:58:56.143116   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0321 12:58:56.456056   12136 main.go:141] libmachine: Using SSH client type: native
I0321 12:58:56.456568   12136 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xbf47e0] 0xbf7320 <nil>  [] 0s} 127.0.0.1 53789 <nil> <nil>}
I0321 12:58:56.456568   12136 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0321 12:58:56.460617   12136 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0321 12:58:59.740177   12136 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0321 12:58:59.771289   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0321 12:59:00.121530   12136 main.go:141] libmachine: Using SSH client type: native
I0321 12:59:00.122755   12136 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xbf47e0] 0xbf7320 <nil>  [] 0s} 127.0.0.1 53789 <nil> <nil>}
I0321 12:59:00.122755   12136 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0321 12:59:00.321953   12136 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0321 12:59:00.321953   12136 ubuntu.go:175] set auth options {CertDir:C:\Users\Nana Brown\.minikube CaCertPath:C:\Users\Nana Brown\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Nana Brown\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Nana Brown\.minikube\machines\server.pem ServerKeyPath:C:\Users\Nana Brown\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Nana Brown\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Nana Brown\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Nana Brown\.minikube}
I0321 12:59:00.321953   12136 ubuntu.go:177] setting up certificates
I0321 12:59:00.321953   12136 provision.go:83] configureAuth start
I0321 12:59:00.345693   12136 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0321 12:59:00.555844   12136 provision.go:138] copyHostCerts
I0321 12:59:00.556440   12136 exec_runner.go:144] found C:\Users\Nana Brown\.minikube/cert.pem, removing ...
I0321 12:59:00.556950   12136 exec_runner.go:203] rm: C:\Users\Nana Brown\.minikube\cert.pem
I0321 12:59:00.557043   12136 exec_runner.go:151] cp: C:\Users\Nana Brown\.minikube\certs\cert.pem --> C:\Users\Nana Brown\.minikube/cert.pem (1131 bytes)
I0321 12:59:00.558145   12136 exec_runner.go:144] found C:\Users\Nana Brown\.minikube/key.pem, removing ...
I0321 12:59:00.558145   12136 exec_runner.go:203] rm: C:\Users\Nana Brown\.minikube\key.pem
I0321 12:59:00.558145   12136 exec_runner.go:151] cp: C:\Users\Nana Brown\.minikube\certs\key.pem --> C:\Users\Nana Brown\.minikube/key.pem (1675 bytes)
I0321 12:59:00.560017   12136 exec_runner.go:144] found C:\Users\Nana Brown\.minikube/ca.pem, removing ...
I0321 12:59:00.560017   12136 exec_runner.go:203] rm: C:\Users\Nana Brown\.minikube\ca.pem
I0321 12:59:00.560384   12136 exec_runner.go:151] cp: C:\Users\Nana Brown\.minikube\certs\ca.pem --> C:\Users\Nana Brown\.minikube/ca.pem (1086 bytes)
I0321 12:59:00.560963   12136 provision.go:112] generating server cert: C:\Users\Nana Brown\.minikube\machines\server.pem ca-key=C:\Users\Nana Brown\.minikube\certs\ca.pem private-key=C:\Users\Nana Brown\.minikube\certs\ca-key.pem org=Nana Brown.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0321 12:59:00.738909   12136 provision.go:172] copyRemoteCerts
I0321 12:59:00.771083   12136 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0321 12:59:00.793325   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0321 12:59:01.067887   12136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53789 SSHKeyPath:C:\Users\Nana Brown\.minikube\machines\minikube\id_rsa Username:docker}
I0321 12:59:01.210038   12136 ssh_runner.go:362] scp C:\Users\Nana Brown\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0321 12:59:01.278128   12136 ssh_runner.go:362] scp C:\Users\Nana Brown\.minikube\machines\server.pem --> /etc/docker/server.pem (1212 bytes)
I0321 12:59:01.336345   12136 ssh_runner.go:362] scp C:\Users\Nana Brown\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0321 12:59:01.393929   12136 provision.go:86] duration metric: configureAuth took 1.0719757s
I0321 12:59:01.393929   12136 ubuntu.go:193] setting minikube options for container-runtime
I0321 12:59:01.395048   12136 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0321 12:59:01.418829   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0321 12:59:01.649487   12136 main.go:141] libmachine: Using SSH client type: native
I0321 12:59:01.650078   12136 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xbf47e0] 0xbf7320 <nil>  [] 0s} 127.0.0.1 53789 <nil> <nil>}
I0321 12:59:01.650078   12136 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0321 12:59:01.833481   12136 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0321 12:59:01.833481   12136 ubuntu.go:71] root file system type: overlay
I0321 12:59:01.833481   12136 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0321 12:59:01.853034   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0321 12:59:02.159364   12136 main.go:141] libmachine: Using SSH client type: native
I0321 12:59:02.160602   12136 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xbf47e0] 0xbf7320 <nil>  [] 0s} 127.0.0.1 53789 <nil> <nil>}
I0321 12:59:02.160602   12136 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0321 12:59:02.343960   12136 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0321 12:59:02.363829   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0321 12:59:02.641323   12136 main.go:141] libmachine: Using SSH client type: native
I0321 12:59:02.642629   12136 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xbf47e0] 0xbf7320 <nil>  [] 0s} 127.0.0.1 53789 <nil> <nil>}
I0321 12:59:02.642629   12136 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0321 12:59:02.833325   12136 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0321 12:59:02.833325   12136 machine.go:91] provisioned docker machine in 6.7081745s
I0321 12:59:02.833325   12136 start.go:300] post-start starting for "minikube" (driver="docker")
I0321 12:59:02.833325   12136 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0321 12:59:02.872220   12136 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0321 12:59:02.895089   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0321 12:59:03.128705   12136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53789 SSHKeyPath:C:\Users\Nana Brown\.minikube\machines\minikube\id_rsa Username:docker}
I0321 12:59:03.274818   12136 ssh_runner.go:195] Run: cat /etc/os-release
I0321 12:59:03.286080   12136 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0321 12:59:03.286080   12136 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0321 12:59:03.286080   12136 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0321 12:59:03.286080   12136 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0321 12:59:03.286080   12136 filesync.go:126] Scanning C:\Users\Nana Brown\.minikube\addons for local assets ...
I0321 12:59:03.286646   12136 filesync.go:126] Scanning C:\Users\Nana Brown\.minikube\files for local assets ...
I0321 12:59:03.287327   12136 start.go:303] post-start completed in 454.0022ms
I0321 12:59:03.290896   12136 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0321 12:59:03.314165   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0321 12:59:03.559132   12136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53789 SSHKeyPath:C:\Users\Nana Brown\.minikube\machines\minikube\id_rsa Username:docker}
I0321 12:59:03.658214   12136 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0321 12:59:03.665718   12136 fix.go:56] fixHost completed within 9.5482004s
I0321 12:59:03.665718   12136 start.go:83] releasing machines lock for "minikube", held for 9.548332s
I0321 12:59:03.678560   12136 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0321 12:59:03.854059   12136 ssh_runner.go:195] Run: cat /version.json
I0321 12:59:03.854674   12136 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0321 12:59:03.866886   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0321 12:59:03.867398   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0321 12:59:04.036467   12136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53789 SSHKeyPath:C:\Users\Nana Brown\.minikube\machines\minikube\id_rsa Username:docker}
I0321 12:59:04.051708   12136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53789 SSHKeyPath:C:\Users\Nana Brown\.minikube\machines\minikube\id_rsa Username:docker}
I0321 12:59:04.162889   12136 ssh_runner.go:195] Run: systemctl --version
I0321 12:59:04.514479   12136 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0321 12:59:04.543014   12136 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0321 12:59:04.558473   12136 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0321 12:59:04.577308   12136 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0321 12:59:04.589659   12136 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0321 12:59:04.589659   12136 start.go:472] detecting cgroup driver to use...
I0321 12:59:04.589659   12136 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0321 12:59:04.589659   12136 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0321 12:59:04.612737   12136 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0321 12:59:04.630838   12136 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0321 12:59:04.646907   12136 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0321 12:59:04.649195   12136 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0321 12:59:04.666346   12136 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0321 12:59:04.682467   12136 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0321 12:59:04.698511   12136 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0321 12:59:04.715086   12136 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0321 12:59:04.731489   12136 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0321 12:59:04.769091   12136 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0321 12:59:04.813281   12136 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0321 12:59:04.873879   12136 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0321 12:59:05.102141   12136 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0321 12:59:05.322293   12136 start.go:472] detecting cgroup driver to use...
I0321 12:59:05.322293   12136 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0321 12:59:05.365142   12136 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0321 12:59:05.400457   12136 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0321 12:59:05.445688   12136 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0321 12:59:05.494120   12136 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0321 12:59:05.580667   12136 ssh_runner.go:195] Run: which cri-dockerd
I0321 12:59:05.720091   12136 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0321 12:59:05.748265   12136 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0321 12:59:05.927717   12136 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0321 12:59:06.235134   12136 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0321 12:59:06.497822   12136 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0321 12:59:06.497822   12136 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0321 12:59:06.572803   12136 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0321 12:59:06.739117   12136 ssh_runner.go:195] Run: sudo systemctl restart docker
I0321 12:59:07.612523   12136 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0321 12:59:07.864469   12136 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0321 12:59:08.138362   12136 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0321 12:59:08.290638   12136 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0321 12:59:08.503420   12136 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0321 12:59:08.560384   12136 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0321 12:59:08.723359   12136 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0321 12:59:09.228255   12136 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0321 12:59:09.235086   12136 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0321 12:59:09.246934   12136 start.go:540] Will wait 60s for crictl version
I0321 12:59:09.251480   12136 ssh_runner.go:195] Run: which crictl
I0321 12:59:09.300183   12136 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0321 12:59:09.618978   12136 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0321 12:59:09.631261   12136 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0321 12:59:09.813861   12136 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0321 12:59:09.854174   12136 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0321 12:59:09.868034   12136 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0321 12:59:10.190395   12136 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0321 12:59:10.192702   12136 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0321 12:59:10.199787   12136 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0321 12:59:10.230431   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0321 12:59:10.420685   12136 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0321 12:59:10.436801   12136 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0321 12:59:10.466661   12136 docker.go:671] Got preloaded images: -- stdout --
thinkai/k8s-web-hello:latest
nginx:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0321 12:59:10.466661   12136 docker.go:601] Images already preloaded, skipping extraction
I0321 12:59:10.482126   12136 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0321 12:59:10.522890   12136 docker.go:671] Got preloaded images: -- stdout --
thinkai/k8s-web-hello:latest
nginx:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0321 12:59:10.522890   12136 cache_images.go:84] Images are preloaded, skipping loading
I0321 12:59:10.542577   12136 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0321 12:59:11.131177   12136 cni.go:84] Creating CNI manager for ""
I0321 12:59:11.131177   12136 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0321 12:59:11.131177   12136 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0321 12:59:11.131177   12136 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0321 12:59:11.131177   12136 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0321 12:59:11.131710   12136 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0321 12:59:11.151782   12136 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0321 12:59:11.165677   12136 binaries.go:44] Found k8s binaries, skipping transfer
I0321 12:59:11.184527   12136 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0321 12:59:11.201601   12136 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0321 12:59:11.226108   12136 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0321 12:59:11.252645   12136 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0321 12:59:11.287967   12136 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0321 12:59:11.293969   12136 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0321 12:59:11.311906   12136 certs.go:56] Setting up C:\Users\Nana Brown\.minikube\profiles\minikube for IP: 192.168.49.2
I0321 12:59:11.311906   12136 certs.go:190] acquiring lock for shared ca certs: {Name:mk7cc677da484229d362c704fa150a06c315fdb4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0321 12:59:11.312490   12136 certs.go:199] skipping minikubeCA CA generation: C:\Users\Nana Brown\.minikube\ca.key
I0321 12:59:11.313528   12136 certs.go:199] skipping proxyClientCA CA generation: C:\Users\Nana Brown\.minikube\proxy-client-ca.key
I0321 12:59:11.314103   12136 certs.go:315] skipping minikube-user signed cert generation: C:\Users\Nana Brown\.minikube\profiles\minikube\client.key
I0321 12:59:11.315173   12136 certs.go:315] skipping minikube signed cert generation: C:\Users\Nana Brown\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I0321 12:59:11.315740   12136 certs.go:315] skipping aggregator signed cert generation: C:\Users\Nana Brown\.minikube\profiles\minikube\proxy-client.key
I0321 12:59:11.317390   12136 certs.go:437] found cert: C:\Users\Nana Brown\.minikube\certs\C:\Users\Nana Brown\.minikube\certs\ca-key.pem (1679 bytes)
I0321 12:59:11.317390   12136 certs.go:437] found cert: C:\Users\Nana Brown\.minikube\certs\C:\Users\Nana Brown\.minikube\certs\ca.pem (1086 bytes)
I0321 12:59:11.317390   12136 certs.go:437] found cert: C:\Users\Nana Brown\.minikube\certs\C:\Users\Nana Brown\.minikube\certs\cert.pem (1131 bytes)
I0321 12:59:11.317390   12136 certs.go:437] found cert: C:\Users\Nana Brown\.minikube\certs\C:\Users\Nana Brown\.minikube\certs\key.pem (1675 bytes)
I0321 12:59:11.319069   12136 ssh_runner.go:362] scp C:\Users\Nana Brown\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0321 12:59:11.351750   12136 ssh_runner.go:362] scp C:\Users\Nana Brown\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0321 12:59:11.385332   12136 ssh_runner.go:362] scp C:\Users\Nana Brown\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0321 12:59:11.422741   12136 ssh_runner.go:362] scp C:\Users\Nana Brown\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0321 12:59:11.456053   12136 ssh_runner.go:362] scp C:\Users\Nana Brown\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0321 12:59:11.506433   12136 ssh_runner.go:362] scp C:\Users\Nana Brown\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0321 12:59:11.564305   12136 ssh_runner.go:362] scp C:\Users\Nana Brown\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0321 12:59:11.628268   12136 ssh_runner.go:362] scp C:\Users\Nana Brown\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0321 12:59:11.687259   12136 ssh_runner.go:362] scp C:\Users\Nana Brown\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0321 12:59:11.745809   12136 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0321 12:59:11.794756   12136 ssh_runner.go:195] Run: openssl version
I0321 12:59:11.904121   12136 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0321 12:59:11.963097   12136 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0321 12:59:11.984513   12136 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Mar 19 18:39 /usr/share/ca-certificates/minikubeCA.pem
I0321 12:59:11.989782   12136 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0321 12:59:12.090207   12136 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0321 12:59:12.140594   12136 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0321 12:59:12.164876   12136 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0321 12:59:12.207034   12136 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0321 12:59:12.249695   12136 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0321 12:59:12.285481   12136 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0321 12:59:12.320637   12136 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0321 12:59:12.357216   12136 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0321 12:59:12.399962   12136 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:8100 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Nana Brown:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0321 12:59:12.443409   12136 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0321 12:59:12.613023   12136 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0321 12:59:12.657102   12136 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0321 12:59:12.657102   12136 kubeadm.go:636] restartCluster start
I0321 12:59:12.735036   12136 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0321 12:59:12.775951   12136 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0321 12:59:12.822508   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0321 12:59:13.249537   12136 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:62620"
I0321 12:59:13.249537   12136 kubeconfig.go:135] verify returned: got: 127.0.0.1:62620, want: 127.0.0.1:53788
I0321 12:59:13.251539   12136 lock.go:35] WriteFile acquiring C:\Users\Nana Brown\.kube\config: {Name:mk2adaf2a0edd6ffb0817d6e12845d4b9fdba2a8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0321 12:59:13.320020   12136 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0321 12:59:13.361173   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:13.433092   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:13.490473   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:13.490473   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:13.560356   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:13.604483   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:14.118426   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:14.164194   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:14.198177   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:14.617077   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:14.647116   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:14.663791   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:15.114749   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:15.201369   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:15.260288   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:15.605057   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:15.640129   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:15.664616   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:16.111098   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:16.163358   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:16.209102   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:16.605936   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:16.646543   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:16.675633   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:17.118785   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:17.143894   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:17.162787   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:17.614396   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:17.647793   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:17.669350   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:18.109387   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:18.136648   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:18.155554   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:18.607260   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:18.649861   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:18.680622   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:19.117017   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:19.163595   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:19.196489   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:19.615076   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:19.640599   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:19.657337   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:20.114130   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:20.154778   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:20.182983   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:20.610181   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:20.671256   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:20.714626   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:21.109705   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:21.167292   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:21.206670   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:21.611157   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:21.662186   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:21.688736   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:22.107719   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:22.144972   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:22.162127   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:22.605354   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:22.640441   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:22.664472   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:23.120079   12136 api_server.go:166] Checking apiserver status ...
I0321 12:59:23.163849   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0321 12:59:23.178424   12136 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0321 12:59:23.367931   12136 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0321 12:59:23.367931   12136 kubeadm.go:1128] stopping kube-system containers ...
I0321 12:59:23.397823   12136 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0321 12:59:23.467556   12136 docker.go:469] Stopping containers: [fbecf493e4b3 9db6d5767619 d395b1f53500 4be85200df3d 6a47243e437b a3f86818c4d1 5c186da7560f 71a964a2b354 6ad430f1359c 2e6163e62ef0 fa6a0ae557c8 92d59bd26fbf 8b36d53ce9c6 4a6b52bdd5be 937a500a5400 b3f3b52772f1 9aa6280a8348 7d4afd0b3367 2ff74165f80c 86c3b2107d70 e202ef07dd67 b380769af55e 511b5c305aa5 56503db9af06 adc95402811b baf80fbdaeb0 162fa1547e9b]
I0321 12:59:23.492016   12136 ssh_runner.go:195] Run: docker stop fbecf493e4b3 9db6d5767619 d395b1f53500 4be85200df3d 6a47243e437b a3f86818c4d1 5c186da7560f 71a964a2b354 6ad430f1359c 2e6163e62ef0 fa6a0ae557c8 92d59bd26fbf 8b36d53ce9c6 4a6b52bdd5be 937a500a5400 b3f3b52772f1 9aa6280a8348 7d4afd0b3367 2ff74165f80c 86c3b2107d70 e202ef07dd67 b380769af55e 511b5c305aa5 56503db9af06 adc95402811b baf80fbdaeb0 162fa1547e9b
I0321 12:59:23.568564   12136 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0321 12:59:23.620609   12136 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0321 12:59:23.637311   12136 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Mar 19 19:00 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Mar 20 04:13 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Mar 19 19:00 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Mar 20 04:13 /etc/kubernetes/scheduler.conf

I0321 12:59:23.656724   12136 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0321 12:59:23.691344   12136 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0321 12:59:23.724093   12136 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0321 12:59:23.738977   12136 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0321 12:59:23.765711   12136 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0321 12:59:23.806232   12136 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0321 12:59:23.825133   12136 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0321 12:59:23.850878   12136 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0321 12:59:23.898878   12136 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0321 12:59:23.919929   12136 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0321 12:59:23.919929   12136 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0321 12:59:24.352701   12136 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0321 12:59:26.858337   12136 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (2.5050784s)
I0321 12:59:26.858337   12136 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0321 12:59:27.213733   12136 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0321 12:59:27.355367   12136 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0321 12:59:27.475044   12136 api_server.go:52] waiting for apiserver process to appear ...
I0321 12:59:27.523200   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:27.637330   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:28.290757   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:28.784854   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:29.257829   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:29.769783   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:30.303801   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:30.758694   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:31.282661   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:31.752880   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:32.278684   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:32.772738   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:33.266999   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:33.752622   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:34.276502   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:34.780965   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 12:59:34.962854   12136 api_server.go:72] duration metric: took 7.4878095s to wait for apiserver process to appear ...
I0321 12:59:34.962854   12136 api_server.go:88] waiting for apiserver healthz status ...
I0321 12:59:34.962854   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:34.978020   12136 api_server.go:269] stopped: https://127.0.0.1:53788/healthz: Get "https://127.0.0.1:53788/healthz": EOF
I0321 12:59:34.978020   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:34.988006   12136 api_server.go:269] stopped: https://127.0.0.1:53788/healthz: Get "https://127.0.0.1:53788/healthz": EOF
I0321 12:59:35.496119   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:35.501538   12136 api_server.go:269] stopped: https://127.0.0.1:53788/healthz: Get "https://127.0.0.1:53788/healthz": EOF
I0321 12:59:35.989753   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:35.994387   12136 api_server.go:269] stopped: https://127.0.0.1:53788/healthz: Get "https://127.0.0.1:53788/healthz": EOF
I0321 12:59:36.499678   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:36.507469   12136 api_server.go:269] stopped: https://127.0.0.1:53788/healthz: Get "https://127.0.0.1:53788/healthz": EOF
I0321 12:59:37.001879   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:42.009043   12136 api_server.go:269] stopped: https://127.0.0.1:53788/healthz: Get "https://127.0.0.1:53788/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0321 12:59:42.009043   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:44.078861   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0321 12:59:44.078918   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0321 12:59:44.078918   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:44.177343   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0321 12:59:44.177901   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0321 12:59:44.495640   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:44.579845   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0321 12:59:44.579845   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0321 12:59:44.992896   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:45.064530   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0321 12:59:45.064530   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0321 12:59:45.493637   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:45.586242   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0321 12:59:45.586242   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0321 12:59:45.988706   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:46.078993   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0321 12:59:46.078993   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0321 12:59:46.496882   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:46.668973   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0321 12:59:46.669140   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0321 12:59:46.995226   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:47.085923   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0321 12:59:47.085923   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0321 12:59:47.493943   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:47.672805   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0321 12:59:47.672863   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0321 12:59:48.002005   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:48.094706   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0321 12:59:48.094706   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0321 12:59:48.498520   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:48.581832   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0321 12:59:48.581832   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0321 12:59:48.996314   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:49.077835   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0321 12:59:49.077835   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0321 12:59:49.493910   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:49.566488   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0321 12:59:49.566488   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0321 12:59:49.994314   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:50.083749   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0321 12:59:50.083749   12136 api_server.go:103] status: https://127.0.0.1:53788/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0321 12:59:50.504033   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 12:59:50.670216   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 200:
ok
I0321 12:59:50.774805   12136 api_server.go:141] control plane version: v1.28.3
I0321 12:59:50.774805   12136 api_server.go:131] duration metric: took 15.8119516s to wait for apiserver health ...
I0321 12:59:50.774805   12136 cni.go:84] Creating CNI manager for ""
I0321 12:59:50.774805   12136 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0321 12:59:50.777860   12136 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0321 12:59:50.862991   12136 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0321 12:59:51.269851   12136 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0321 12:59:51.782213   12136 system_pods.go:43] waiting for kube-system pods to appear ...
I0321 12:59:51.899043   12136 system_pods.go:59] 7 kube-system pods found
I0321 12:59:51.899043   12136 system_pods.go:61] "coredns-5dd5756b68-fnpx2" [db4ac241-1038-4e5a-ad7b-c5468f472847] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0321 12:59:51.899043   12136 system_pods.go:61] "etcd-minikube" [3134fefa-e537-4202-80ea-8945ae2de78b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0321 12:59:51.899043   12136 system_pods.go:61] "kube-apiserver-minikube" [3f65ee21-00fd-4e2b-9539-b22231f26833] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0321 12:59:51.899043   12136 system_pods.go:61] "kube-controller-manager-minikube" [84362158-8e1a-4757-a1ee-8bf0a04ab44d] Running
I0321 12:59:51.899043   12136 system_pods.go:61] "kube-proxy-gjkcb" [9aed65af-00be-417f-ba83-b3fcb8b80c96] Running
I0321 12:59:51.899043   12136 system_pods.go:61] "kube-scheduler-minikube" [2765411c-3726-4ec8-a64b-45c152e5249f] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0321 12:59:51.899043   12136 system_pods.go:61] "storage-provisioner" [30d0df46-88fc-4d6d-82e4-a8b6a51710bf] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0321 12:59:51.899043   12136 system_pods.go:74] duration metric: took 116.8299ms to wait for pod list to return data ...
I0321 12:59:51.899043   12136 node_conditions.go:102] verifying NodePressure condition ...
I0321 12:59:51.969952   12136 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0321 12:59:51.970470   12136 node_conditions.go:123] node cpu capacity is 12
I0321 12:59:51.970531   12136 node_conditions.go:105] duration metric: took 71.4881ms to run NodePressure ...
I0321 12:59:51.970531   12136 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0321 12:59:52.989752   12136 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (1.0192207s)
I0321 12:59:52.989752   12136 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0321 12:59:53.084873   12136 ops.go:34] apiserver oom_adj: -16
I0321 12:59:53.084873   12136 kubeadm.go:640] restartCluster took 40.4277714s
I0321 12:59:53.084873   12136 kubeadm.go:406] StartCluster complete in 40.6849113s
I0321 12:59:53.085098   12136 settings.go:142] acquiring lock: {Name:mk197966dfa3b64c693d8fafd012e75158219898 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0321 12:59:53.085098   12136 settings.go:150] Updating kubeconfig:  C:\Users\Nana Brown\.kube\config
I0321 12:59:53.089533   12136 lock.go:35] WriteFile acquiring C:\Users\Nana Brown\.kube\config: {Name:mk2adaf2a0edd6ffb0817d6e12845d4b9fdba2a8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0321 12:59:53.092746   12136 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0321 12:59:53.093291   12136 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0321 12:59:53.093291   12136 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0321 12:59:53.093291   12136 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0321 12:59:53.093291   12136 addons.go:240] addon storage-provisioner should already be in state true
I0321 12:59:53.094312   12136 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0321 12:59:53.094312   12136 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0321 12:59:53.095708   12136 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0321 12:59:53.101136   12136 host.go:66] Checking if "minikube" exists ...
I0321 12:59:53.172625   12136 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0321 12:59:53.172731   12136 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0321 12:59:53.174497   12136 out.go:177] üîé  Verifying Kubernetes components...
I0321 12:59:53.211957   12136 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0321 12:59:53.213817   12136 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0321 12:59:53.255773   12136 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0321 12:59:53.516357   12136 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0321 12:59:53.517497   12136 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0321 12:59:53.517497   12136 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0321 12:59:53.529747   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0321 12:59:53.531956   12136 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0321 12:59:53.531956   12136 addons.go:240] addon default-storageclass should already be in state true
I0321 12:59:53.531956   12136 host.go:66] Checking if "minikube" exists ...
I0321 12:59:53.562376   12136 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0321 12:59:53.761368   12136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53789 SSHKeyPath:C:\Users\Nana Brown\.minikube\machines\minikube\id_rsa Username:docker}
I0321 12:59:53.791678   12136 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0321 12:59:53.791678   12136 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0321 12:59:53.817470   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0321 12:59:54.175249   12136 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53789 SSHKeyPath:C:\Users\Nana Brown\.minikube\machines\minikube\id_rsa Username:docker}
I0321 12:59:54.354082   12136 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0321 12:59:55.198339   12136 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0321 12:59:55.280612   12136 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (2.1877611s)
I0321 12:59:55.280612   12136 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (2.0248389s)
I0321 12:59:55.281182   12136 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0321 12:59:55.327590   12136 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0321 12:59:55.783925   12136 api_server.go:52] waiting for apiserver process to appear ...
I0321 12:59:55.855458   12136 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0321 13:00:01.463294   12136 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (7.1092126s)
I0321 13:00:01.463294   12136 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (6.2649552s)
I0321 13:00:01.463294   12136 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (5.6078359s)
I0321 13:00:01.463294   12136 api_server.go:72] duration metric: took 8.2904602s to wait for apiserver process to appear ...
I0321 13:00:01.463294   12136 api_server.go:88] waiting for apiserver healthz status ...
I0321 13:00:01.463294   12136 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53788/healthz ...
I0321 13:00:01.561013   12136 api_server.go:279] https://127.0.0.1:53788/healthz returned 200:
ok
I0321 13:00:01.562204   12136 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0321 13:00:01.563356   12136 addons.go:502] enable addons completed in 8.4700646s: enabled=[storage-provisioner default-storageclass]
I0321 13:00:01.564513   12136 api_server.go:141] control plane version: v1.28.3
I0321 13:00:01.564513   12136 api_server.go:131] duration metric: took 101.2191ms to wait for apiserver health ...
I0321 13:00:01.564513   12136 system_pods.go:43] waiting for kube-system pods to appear ...
I0321 13:00:01.575250   12136 system_pods.go:59] 7 kube-system pods found
I0321 13:00:01.575250   12136 system_pods.go:61] "coredns-5dd5756b68-fnpx2" [db4ac241-1038-4e5a-ad7b-c5468f472847] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0321 13:00:01.575250   12136 system_pods.go:61] "etcd-minikube" [3134fefa-e537-4202-80ea-8945ae2de78b] Running
I0321 13:00:01.575250   12136 system_pods.go:61] "kube-apiserver-minikube" [3f65ee21-00fd-4e2b-9539-b22231f26833] Running
I0321 13:00:01.575250   12136 system_pods.go:61] "kube-controller-manager-minikube" [84362158-8e1a-4757-a1ee-8bf0a04ab44d] Running
I0321 13:00:01.575250   12136 system_pods.go:61] "kube-proxy-gjkcb" [9aed65af-00be-417f-ba83-b3fcb8b80c96] Running
I0321 13:00:01.575250   12136 system_pods.go:61] "kube-scheduler-minikube" [2765411c-3726-4ec8-a64b-45c152e5249f] Running
I0321 13:00:01.575250   12136 system_pods.go:61] "storage-provisioner" [30d0df46-88fc-4d6d-82e4-a8b6a51710bf] Running
I0321 13:00:01.575250   12136 system_pods.go:74] duration metric: took 10.7363ms to wait for pod list to return data ...
I0321 13:00:01.575250   12136 kubeadm.go:581] duration metric: took 8.4024156s to wait for : map[apiserver:true system_pods:true] ...
I0321 13:00:01.575250   12136 node_conditions.go:102] verifying NodePressure condition ...
I0321 13:00:01.580706   12136 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0321 13:00:01.580706   12136 node_conditions.go:123] node cpu capacity is 12
I0321 13:00:01.580706   12136 node_conditions.go:105] duration metric: took 5.456ms to run NodePressure ...
I0321 13:00:01.580706   12136 start.go:228] waiting for startup goroutines ...
I0321 13:00:01.580706   12136 start.go:233] waiting for cluster config update ...
I0321 13:00:01.580706   12136 start.go:242] writing updated cluster config ...
I0321 13:00:01.584802   12136 ssh_runner.go:195] Run: rm -f paused
I0321 13:00:02.143005   12136 start.go:600] kubectl: 1.26.14-dispatcher, cluster: 1.28.3 (minor skew: 2)
I0321 13:00:02.144147   12136 out.go:177] 
W0321 13:00:02.146428   12136 out.go:239] ‚ùó  C:\Users\Nana Brown\AppData\Local\cloud-code\installer\google-cloud-sdk\bin\kubectl.exe is version 1.26.14-dispatcher, which may have incompatibilities with Kubernetes 1.28.3.
I0321 13:00:02.148416   12136 out.go:177]     ‚ñ™ Want kubectl v1.28.3? Try 'minikube kubectl -- get pods -A'
I0321 13:00:02.150356   12136 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Mar 21 17:28:57 minikube cri-dockerd[1227]: time="2024-03-21T17:28:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"k8s-web-hello-deploy-64846d8668-ztrlh_default\": unexpected command output nsenter: cannot open /proc/33881/ns/net: No such file or directory\n with error: exit status 1"
Mar 21 17:28:57 minikube dockerd[975]: time="2024-03-21T17:28:57.666088202Z" level=info msg="ignoring event" container=25c704b973d38aaccce1a020d9f87906ec4a43305c7b0a2dd17b0cd55ff5b049 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:28:57 minikube cri-dockerd[1227]: time="2024-03-21T17:28:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7dafbb47fb682fc6e7c4d1ffc61282c3489d2d3ba63c0917271cf60441f68cb8/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 17:28:58 minikube dockerd[975]: time="2024-03-21T17:28:58.088237437Z" level=info msg="ignoring event" container=97a7bb1bbecabce6bc06e875df3399dffc88ea9fbc33842066f6ddeb223e3170 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:28:58 minikube cri-dockerd[1227]: time="2024-03-21T17:28:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f837ec3484946e1602757fc3ce9054a669285c3ef61ebe899f1926a632fa82f7/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 17:28:58 minikube cri-dockerd[1227]: time="2024-03-21T17:28:58Z" level=info msg="Stop pulling image thinkai/k8s-web-hello:latest: Status: Image is up to date for thinkai/k8s-web-hello:latest"
Mar 21 17:28:59 minikube cri-dockerd[1227]: time="2024-03-21T17:28:59Z" level=info msg="Stop pulling image thinkai/k8s-web-hello:latest: Status: Image is up to date for thinkai/k8s-web-hello:latest"
Mar 21 17:29:00 minikube dockerd[975]: time="2024-03-21T17:29:00.089963388Z" level=info msg="ignoring event" container=4d3378617687e0a6215ee819fc9273f6a7218325b6e4df6d6f195880615ca88f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:29:00 minikube dockerd[975]: time="2024-03-21T17:29:00.572592957Z" level=info msg="ignoring event" container=a88d24e8394cdbbcbf75a92c8333bb5ba96a601cf30796e4864208d384062c81 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:31:38 minikube dockerd[975]: time="2024-03-21T17:31:38.961124596Z" level=info msg="ignoring event" container=a40a3a4d5dce7724ec821e4786c95e960a6c99d8123bd2da6d3a8fa4da65ee6a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:31:38 minikube cri-dockerd[1227]: time="2024-03-21T17:31:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ce8d0b71520183e9f390f3a110a78aec12f0490294aad104f41e2027c24c9e58/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 17:31:39 minikube dockerd[975]: time="2024-03-21T17:31:39.214689591Z" level=info msg="ignoring event" container=7dafbb47fb682fc6e7c4d1ffc61282c3489d2d3ba63c0917271cf60441f68cb8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:31:39 minikube cri-dockerd[1227]: time="2024-03-21T17:31:39Z" level=info msg="Stop pulling image thinkai/k8s-web-hello:latest: Status: Image is up to date for thinkai/k8s-web-hello:latest"
Mar 21 17:33:24 minikube cri-dockerd[1227]: time="2024-03-21T17:33:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eb1df2c25a6130eea4719ac77dc9053deaf69147f0ebbb56ca6f606d6196846a/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 17:33:25 minikube cri-dockerd[1227]: time="2024-03-21T17:33:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/79943abc16cbc7c1b1716b84871334063c8c3dd309a24a48cfb34af1445ae107/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 17:33:25 minikube dockerd[975]: time="2024-03-21T17:33:25.263982957Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Mar 21 17:33:35 minikube cri-dockerd[1227]: time="2024-03-21T17:33:35Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Extracting [===========================>                       ]  42.34MB/75.78MB"
Mar 21 17:33:40 minikube cri-dockerd[1227]: time="2024-03-21T17:33:40Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: Status: Downloaded newer image for kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Mar 21 17:33:40 minikube dockerd[975]: time="2024-03-21T17:33:40.587028672Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Mar 21 17:33:44 minikube cri-dockerd[1227]: time="2024-03-21T17:33:44Z" level=info msg="Stop pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: Status: Downloaded newer image for kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Mar 21 17:45:39 minikube dockerd[975]: time="2024-03-21T17:45:39.768868408Z" level=info msg="ignoring event" container=42260d7bd9f63523c18e4e4419e14e6d46e8fa85d81b31e84937ceaa12f71611 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:45:39 minikube dockerd[975]: time="2024-03-21T17:45:39.774327298Z" level=info msg="ignoring event" container=dfa7506cebcd7f34c621c7b2e7eab02311814e45dd3beccc217511e126e02c3e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:45:39 minikube dockerd[975]: time="2024-03-21T17:45:39.863940328Z" level=info msg="ignoring event" container=3748a24924e0ae2cb061b4943f898a91e12246783420184bf310b190192f4bd7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:45:39 minikube dockerd[975]: time="2024-03-21T17:45:39.975010218Z" level=info msg="ignoring event" container=f4770acb090322a6f04e142eea2579d13e3f8c62797b02454e81de27375fe0f7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:45:40 minikube cri-dockerd[1227]: time="2024-03-21T17:45:40Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"k8s-web-hello-deploy-7bb449b5d8-8bhbz_default\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Mar 21 17:45:40 minikube dockerd[975]: time="2024-03-21T17:45:40.881420803Z" level=info msg="ignoring event" container=e7402ec17a1f977ff28f5d8b3db00b3153fa96506d97f5ae0d1a90c53da6322b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:45:40 minikube dockerd[975]: time="2024-03-21T17:45:40.964834745Z" level=info msg="ignoring event" container=ce8d0b71520183e9f390f3a110a78aec12f0490294aad104f41e2027c24c9e58 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:45:41 minikube dockerd[975]: time="2024-03-21T17:45:41.075874335Z" level=info msg="ignoring event" container=6874f001c90eaa93cf7cef1446cb4b320c29d7495f262f5564024a214a6243d3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:45:41 minikube dockerd[975]: time="2024-03-21T17:45:41.177451043Z" level=info msg="ignoring event" container=f837ec3484946e1602757fc3ce9054a669285c3ef61ebe899f1926a632fa82f7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:45:41 minikube cri-dockerd[1227]: time="2024-03-21T17:45:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dc545901f3183cdf1d573daddd4657669b274483079e375abde44adb949d569e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 17:45:41 minikube cri-dockerd[1227]: time="2024-03-21T17:45:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bca6bf055d640afa3f40ddd3aacc4fac55999bfe3b35f7eca950b94e6afa888e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 17:45:41 minikube cri-dockerd[1227]: time="2024-03-21T17:45:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ad21a0591926ed586c54179119c47a75dd8e9e66eace296f2221a99be0478bf0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 17:45:41 minikube cri-dockerd[1227]: time="2024-03-21T17:45:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c0640ab6563936cadaf18a990d3a886aaeca2780129bf779394f297c89888387/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 17:45:42 minikube cri-dockerd[1227]: time="2024-03-21T17:45:42Z" level=info msg="Stop pulling image thinkai/k8s-web-hello:latest: Status: Image is up to date for thinkai/k8s-web-hello:latest"
Mar 21 17:45:43 minikube cri-dockerd[1227]: time="2024-03-21T17:45:43Z" level=error msg="Error response from daemon: No such container: dfa7506cebcd7f34c621c7b2e7eab02311814e45dd3beccc217511e126e02c3e Failed to get stats from container dfa7506cebcd7f34c621c7b2e7eab02311814e45dd3beccc217511e126e02c3e"
Mar 21 17:45:43 minikube cri-dockerd[1227]: time="2024-03-21T17:45:43Z" level=info msg="Stop pulling image thinkai/k8s-web-hello:latest: Status: Image is up to date for thinkai/k8s-web-hello:latest"
Mar 21 17:45:44 minikube cri-dockerd[1227]: time="2024-03-21T17:45:44Z" level=info msg="Stop pulling image thinkai/k8s-web-hello:latest: Status: Image is up to date for thinkai/k8s-web-hello:latest"
Mar 21 17:45:45 minikube cri-dockerd[1227]: time="2024-03-21T17:45:45Z" level=info msg="Stop pulling image thinkai/k8s-web-hello:latest: Status: Image is up to date for thinkai/k8s-web-hello:latest"
Mar 21 17:46:14 minikube dockerd[975]: time="2024-03-21T17:46:14.682689229Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=d9f62dbb4220a9677a036c8a1683b18b7dd2e486e9136d8037b45fc1589a0483
Mar 21 17:46:14 minikube dockerd[975]: time="2024-03-21T17:46:14.721232079Z" level=info msg="ignoring event" container=d9f62dbb4220a9677a036c8a1683b18b7dd2e486e9136d8037b45fc1589a0483 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:46:14 minikube dockerd[975]: time="2024-03-21T17:46:14.769888816Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=3c325b2ddb684ba1fa339192a2376a87303843093a479df3445e81e87b690336
Mar 21 17:46:14 minikube dockerd[975]: time="2024-03-21T17:46:14.812810061Z" level=info msg="ignoring event" container=3c325b2ddb684ba1fa339192a2376a87303843093a479df3445e81e87b690336 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:46:14 minikube dockerd[975]: time="2024-03-21T17:46:14.993341528Z" level=info msg="ignoring event" container=ad21a0591926ed586c54179119c47a75dd8e9e66eace296f2221a99be0478bf0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:46:15 minikube dockerd[975]: time="2024-03-21T17:46:15.093474898Z" level=info msg="ignoring event" container=dc545901f3183cdf1d573daddd4657669b274483079e375abde44adb949d569e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:46:15 minikube dockerd[975]: time="2024-03-21T17:46:15.964589418Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=174edc4ea1e485ba96c82ecba12b27865deb981bbf1c2afdab03a7ab6c7bbdd8
Mar 21 17:46:15 minikube dockerd[975]: time="2024-03-21T17:46:15.981799716Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=3934020948a0eb377aa2b956780864ac60c30fd524659cc6c07d13b2cf4eb0df
Mar 21 17:46:16 minikube dockerd[975]: time="2024-03-21T17:46:16.002724215Z" level=info msg="ignoring event" container=174edc4ea1e485ba96c82ecba12b27865deb981bbf1c2afdab03a7ab6c7bbdd8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:46:16 minikube dockerd[975]: time="2024-03-21T17:46:16.019578813Z" level=info msg="ignoring event" container=3934020948a0eb377aa2b956780864ac60c30fd524659cc6c07d13b2cf4eb0df module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:46:16 minikube dockerd[975]: time="2024-03-21T17:46:16.268191092Z" level=info msg="ignoring event" container=c0640ab6563936cadaf18a990d3a886aaeca2780129bf779394f297c89888387 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 17:46:16 minikube dockerd[975]: time="2024-03-21T17:46:16.334153186Z" level=info msg="ignoring event" container=bca6bf055d640afa3f40ddd3aacc4fac55999bfe3b35f7eca950b94e6afa888e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 21 18:12:01 minikube cri-dockerd[1227]: time="2024-03-21T18:12:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3fe86f1dbaa75f233f396f7ca979aca3641df6858c78d211a947732d7f4289d0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 18:12:02 minikube cri-dockerd[1227]: time="2024-03-21T18:12:02Z" level=info msg="Stop pulling image thinkai/k8s-web-hello:latest: Status: Image is up to date for thinkai/k8s-web-hello:latest"
Mar 21 18:47:47 minikube cri-dockerd[1227]: time="2024-03-21T18:47:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bc06ee39f5b40e7f6beb17935e0aa7d42ae77f75895a5df149cf88669a3d95e1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 18:47:48 minikube cri-dockerd[1227]: time="2024-03-21T18:47:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fd6c20cfc9d69964c179d0c2041a94f9f5e6a890cc8d541db282ad6f8481d27f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 18:47:48 minikube cri-dockerd[1227]: time="2024-03-21T18:47:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2a89f2a86b26fc8bba27a4a7c8519ddd092f549fe7ce32ff8e9706cbc95a1e2e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 18:47:48 minikube cri-dockerd[1227]: time="2024-03-21T18:47:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f7a77870323e8b9e864adb84796b8e7832c5a6cbc85f0f45b86c0942fae3838f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 21 18:47:49 minikube cri-dockerd[1227]: time="2024-03-21T18:47:49Z" level=info msg="Stop pulling image thinkai/k8s-web-hello:latest: Status: Image is up to date for thinkai/k8s-web-hello:latest"
Mar 21 18:47:49 minikube cri-dockerd[1227]: time="2024-03-21T18:47:49Z" level=info msg="Stop pulling image thinkai/k8s-web-hello:latest: Status: Image is up to date for thinkai/k8s-web-hello:latest"
Mar 21 18:47:50 minikube cri-dockerd[1227]: time="2024-03-21T18:47:50Z" level=info msg="Stop pulling image thinkai/k8s-web-hello:latest: Status: Image is up to date for thinkai/k8s-web-hello:latest"
Mar 21 18:47:51 minikube cri-dockerd[1227]: time="2024-03-21T18:47:51Z" level=info msg="Stop pulling image thinkai/k8s-web-hello:latest: Status: Image is up to date for thinkai/k8s-web-hello:latest"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
7c037a1915761       thinkai/k8s-web-hello@sha256:1083bff8a56aa76d0a3d97142c03900940554240897c837e2039e97862db4e92          2 hours ago         Running             k8s-web-hello               0                   f7a77870323e8       k8s-web-hello-f6bcdcc9c-cbzwh
e56b6e36424f7       thinkai/k8s-web-hello@sha256:1083bff8a56aa76d0a3d97142c03900940554240897c837e2039e97862db4e92          2 hours ago         Running             k8s-web-hello               0                   2a89f2a86b26f       k8s-web-hello-f6bcdcc9c-vfqqt
de640a6b081d3       thinkai/k8s-web-hello@sha256:1083bff8a56aa76d0a3d97142c03900940554240897c837e2039e97862db4e92          2 hours ago         Running             k8s-web-hello               0                   fd6c20cfc9d69       k8s-web-hello-f6bcdcc9c-c6b9t
3f7dad6555bfa       thinkai/k8s-web-hello@sha256:1083bff8a56aa76d0a3d97142c03900940554240897c837e2039e97862db4e92          2 hours ago         Running             k8s-web-hello               0                   bc06ee39f5b40       k8s-web-hello-f6bcdcc9c-spjw6
c9946de461142       thinkai/k8s-web-hello@sha256:1083bff8a56aa76d0a3d97142c03900940554240897c837e2039e97862db4e92          2 hours ago         Running             k8s-web-hello               0                   3fe86f1dbaa75       k8s-web-hello-f6bcdcc9c-mqxcr
59655a5472334       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   3 hours ago         Running             dashboard-metrics-scraper   0                   79943abc16cbc       dashboard-metrics-scraper-7fd5cb4ddc-jzsgf
bcda0851c513d       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93         3 hours ago         Running             kubernetes-dashboard        0                   eb1df2c25a613       kubernetes-dashboard-8694d4445c-dhrhq
9fb101ca750c4       6e38f40d628db                                                                                          3 hours ago         Running             storage-provisioner         5                   fe32e94d48ad6       storage-provisioner
bcc165c10cc26       ead0a4a53df89                                                                                          3 hours ago         Running             coredns                     2                   e73a533196519       coredns-5dd5756b68-fnpx2
0223ebd917169       6e38f40d628db                                                                                          3 hours ago         Exited              storage-provisioner         4                   fe32e94d48ad6       storage-provisioner
b5c4f1301c38d       bfc896cf80fba                                                                                          3 hours ago         Running             kube-proxy                  2                   0b07e3597e64a       kube-proxy-gjkcb
10771fe52a34d       10baa1ca17068                                                                                          3 hours ago         Running             kube-controller-manager     2                   7cdc785d20809       kube-controller-manager-minikube
fde85ec9ec670       73deb9a3f7025                                                                                          3 hours ago         Running             etcd                        2                   f3e392a1f4603       etcd-minikube
e7ca2677daf7e       6d1b4fd1b182d                                                                                          3 hours ago         Running             kube-scheduler              2                   9e3a78f671f0f       kube-scheduler-minikube
1b85d845b6f06       5374347291230                                                                                          3 hours ago         Running             kube-apiserver              2                   ecc17fb501467       kube-apiserver-minikube
9db6d5767619e       ead0a4a53df89                                                                                          40 hours ago        Exited              coredns                     1                   6a47243e437b6       coredns-5dd5756b68-fnpx2
4be85200df3d4       bfc896cf80fba                                                                                          40 hours ago        Exited              kube-proxy                  1                   a3f86818c4d18       kube-proxy-gjkcb
71a964a2b354e       6d1b4fd1b182d                                                                                          40 hours ago        Exited              kube-scheduler              1                   92d59bd26fbf4       kube-scheduler-minikube
6ad430f1359cd       5374347291230                                                                                          40 hours ago        Exited              kube-apiserver              1                   4a6b52bdd5be2       kube-apiserver-minikube
2e6163e62ef0b       73deb9a3f7025                                                                                          40 hours ago        Exited              etcd                        1                   937a500a54009       etcd-minikube
fa6a0ae557c8c       10baa1ca17068                                                                                          40 hours ago        Exited              kube-controller-manager     1                   8b36d53ce9c6a       kube-controller-manager-minikube

* 
* ==> coredns [9db6d5767619] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:53539 - 43447 "HINFO IN 6883075469422767264.1834363420210755532. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.09861512s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.127919291s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.387588295s

* 
* ==> coredns [bcc165c10cc2] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:59566 - 22577 "HINFO IN 2594919266131019595.3461166586611457891. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.093517408s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_03_19T15_00_30_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 19 Mar 2024 19:00:26 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 21 Mar 2024 20:27:42 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 21 Mar 2024 20:25:46 +0000   Tue, 19 Mar 2024 19:00:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 21 Mar 2024 20:25:46 +0000   Tue, 19 Mar 2024 19:00:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 21 Mar 2024 20:25:46 +0000   Tue, 19 Mar 2024 19:00:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 21 Mar 2024 20:25:46 +0000   Tue, 19 Mar 2024 19:00:40 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16225428Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16225428Ki
  pods:               110
System Info:
  Machine ID:                 65ec5e869f3248778340c44c2ac11123
  System UUID:                65ec5e869f3248778340c44c2ac11123
  Boot ID:                    2bc9f8aa-4160-4f78-a436-58084c024428
  Kernel Version:             5.15.133.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     k8s-web-hello-f6bcdcc9c-c6b9t                 500m (4%!)(MISSING)     500m (4%!)(MISSING)   128Mi (0%!)(MISSING)       128Mi (0%!)(MISSING)     99m
  default                     k8s-web-hello-f6bcdcc9c-cbzwh                 500m (4%!)(MISSING)     500m (4%!)(MISSING)   128Mi (0%!)(MISSING)       128Mi (0%!)(MISSING)     99m
  default                     k8s-web-hello-f6bcdcc9c-mqxcr                 500m (4%!)(MISSING)     500m (4%!)(MISSING)   128Mi (0%!)(MISSING)       128Mi (0%!)(MISSING)     135m
  default                     k8s-web-hello-f6bcdcc9c-spjw6                 500m (4%!)(MISSING)     500m (4%!)(MISSING)   128Mi (0%!)(MISSING)       128Mi (0%!)(MISSING)     99m
  default                     k8s-web-hello-f6bcdcc9c-vfqqt                 500m (4%!)(MISSING)     500m (4%!)(MISSING)   128Mi (0%!)(MISSING)       128Mi (0%!)(MISSING)     99m
  kube-system                 coredns-5dd5756b68-fnpx2                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     2d1h
  kube-system                 etcd-minikube                                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         2d1h
  kube-system                 kube-apiserver-minikube                       250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d1h
  kube-system                 kube-controller-manager-minikube              200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d1h
  kube-system                 kube-proxy-gjkcb                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d1h
  kube-system                 kube-scheduler-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d1h
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d1h
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-jzsgf    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         174m
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-dhrhq         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         174m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                3250m (27%!)(MISSING)  2500m (20%!)(MISSING)
  memory             810Mi (5%!)(MISSING)   810Mi (5%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +0.023857] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.410780] Exception: 
[  +0.000005] Operation canceled @p9io.cpp:258 (AcceptAsync)

[  +0.016664] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.002075] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.001803] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.001942] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.001581] blk_update_request: I/O error, dev sdc, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.001503] blk_update_request: I/O error, dev sdc, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.001538] Buffer I/O error on dev sdc, logical block 134184960, lost sync page write
[  +0.001350] JBD2: Error -5 detected when updating journal superblock for sdc-8.
[  +0.000904] Aborting journal on device sdc-8.
[  +0.000962] blk_update_request: I/O error, dev sdc, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.001376] blk_update_request: I/O error, dev sdc, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.001487] Buffer I/O error on dev sdc, logical block 134184960, lost sync page write
[  +0.001150] JBD2: Error -5 detected when updating journal superblock for sdc-8.
[  +0.001041] EXT4-fs error (device sdc): ext4_put_super:1196: comm weston: Couldn't clean up the journal
[  +0.001338] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x3800 phys_seg 1 prio class 0
[  +0.001638] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x3800 phys_seg 1 prio class 0
[  +0.001413] Buffer I/O error on dev sdc, logical block 0, lost sync page write
[  +0.001111] EXT4-fs (sdc): I/O error while writing superblock
[  +0.000748] EXT4-fs (sdc): Remounting filesystem read-only
[  +4.620553] /sbin/ldconfig: 
[  +0.000016] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.105144] WSL (2) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.004403] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.006414] WSL (1) ERROR: ConfigMountFsTab:2579: Processing fstab with mount -a failed.
[  +0.007672] WSL (1) ERROR: ConfigApplyWindowsLibPath:2527: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000017]  failed 2
[  +0.013765] WSL (3) ERROR: UtilCreateProcessAndWait:665: /bin/mount failed with 2
[  +0.004340] WSL (1) ERROR: UtilCreateProcessAndWait:687: /bin/mount failed with status 0xff00

[  +0.043272] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.257687] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000999] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001051] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001207] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002157] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000962] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001107] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001094] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.073041] /sbin/ldconfig: 
[  +0.000005] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.046066] WSL (1) ERROR: ConfigApplyWindowsLibPath:2527: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000006]  failed 2
[  +0.015033] WSL (1) WARNING: /usr/share/zoneinfo/America/New_York not found. Is the tzdata package installed?
[  +0.174889] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001234] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001303] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002222] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002573] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001181] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001375] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001447] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.363689] netlink: 'init': attribute type 4 has an invalid length.
[  +0.692758] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.

* 
* ==> etcd [2e6163e62ef0] <==
* {"level":"warn","ts":"2024-03-20T05:25:17.823456Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-20T05:25:17.223082Z","time spent":"600.331961ms","remote":"127.0.0.1:35670","response type":"/etcdserverpb.KV/Range","request count":0,"request size":29,"response count":1,"response size":123404,"request content":"key:\"/registry/ranges/serviceips\" "}
{"level":"warn","ts":"2024-03-20T05:25:17.83106Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-20T05:25:17.128746Z","time spent":"692.031395ms","remote":"127.0.0.1:35646","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:23106 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128027944966186128 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2024-03-20T05:25:17.929488Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.989146ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1111"}
{"level":"info","ts":"2024-03-20T05:25:17.929637Z","caller":"traceutil/trace.go:171","msg":"trace[89734713] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:23113; }","duration":"104.142447ms","start":"2024-03-20T05:25:17.825467Z","end":"2024-03-20T05:25:17.929609Z","steps":["trace[89734713] 'range keys from in-memory index tree'  (duration: 103.830944ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-20T05:25:17.929705Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"604.812902ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/runtimeclasses/\" range_end:\"/registry/runtimeclasses0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-03-20T05:25:17.93017Z","caller":"traceutil/trace.go:171","msg":"trace[1106549521] range","detail":"{range_begin:/registry/runtimeclasses/; range_end:/registry/runtimeclasses0; response_count:0; response_revision:23113; }","duration":"604.951903ms","start":"2024-03-20T05:25:17.324863Z","end":"2024-03-20T05:25:17.929815Z","steps":["trace[1106549521] 'agreement among raft nodes before linearized reading'  (duration: 296.625798ms)","trace[1106549521] 'count revisions from in-memory index tree'  (duration: 308.146903ms)"],"step_count":2}
{"level":"warn","ts":"2024-03-20T05:25:17.937059Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-20T05:25:17.324843Z","time spent":"612.174468ms","remote":"127.0.0.1:35936","response type":"/etcdserverpb.KV/Range","request count":0,"request size":56,"response count":0,"response size":30,"request content":"key:\"/registry/runtimeclasses/\" range_end:\"/registry/runtimeclasses0\" count_only:true "}
{"level":"warn","ts":"2024-03-20T05:25:17.930711Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.070592ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-03-20T05:25:18.022106Z","caller":"traceutil/trace.go:171","msg":"trace[225248565] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:23113; }","duration":"200.063919ms","start":"2024-03-20T05:25:17.821619Z","end":"2024-03-20T05:25:18.021683Z","steps":["trace[225248565] 'range keys from in-memory index tree'  (duration: 109.016092ms)"],"step_count":1}
{"level":"info","ts":"2024-03-20T05:25:19.727643Z","caller":"traceutil/trace.go:171","msg":"trace[1780373013] transaction","detail":"{read_only:false; response_revision:23115; number_of_response:1; }","duration":"102.457626ms","start":"2024-03-20T05:25:19.624851Z","end":"2024-03-20T05:25:19.727309Z","steps":["trace[1780373013] 'process raft request'  (duration: 101.694919ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-20T05:25:19.730708Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.824021ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-03-20T05:25:19.730822Z","caller":"traceutil/trace.go:171","msg":"trace[1006743340] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:23115; }","duration":"101.941022ms","start":"2024-03-20T05:25:19.628849Z","end":"2024-03-20T05:25:19.73079Z","steps":["trace[1006743340] 'agreement among raft nodes before linearized reading'  (duration: 97.908485ms)"],"step_count":1}
{"level":"info","ts":"2024-03-20T05:28:12.801281Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23015}
{"level":"info","ts":"2024-03-20T05:28:12.802658Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":23015,"took":"1.16891ms","hash":1720264817}
{"level":"info","ts":"2024-03-20T05:28:12.802716Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1720264817,"revision":23015,"compact-revision":22776}
{"level":"info","ts":"2024-03-20T05:33:12.808968Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23252}
{"level":"info","ts":"2024-03-20T05:33:12.809981Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":23252,"took":"715.602¬µs","hash":530225042}
{"level":"info","ts":"2024-03-20T05:33:12.810029Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":530225042,"revision":23252,"compact-revision":23015}
{"level":"info","ts":"2024-03-20T05:38:12.817979Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23492}
{"level":"info","ts":"2024-03-20T05:38:12.81939Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":23492,"took":"1.180892ms","hash":1116803592}
{"level":"info","ts":"2024-03-20T05:38:12.819482Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1116803592,"revision":23492,"compact-revision":23252}
{"level":"info","ts":"2024-03-20T05:39:48.358731Z","caller":"traceutil/trace.go:171","msg":"trace[65996603] transaction","detail":"{read_only:false; response_revision:23808; number_of_response:1; }","duration":"104.125502ms","start":"2024-03-20T05:39:48.254574Z","end":"2024-03-20T05:39:48.3587Z","steps":["trace[65996603] 'process raft request'  (duration: 103.965402ms)"],"step_count":1}
{"level":"info","ts":"2024-03-20T05:42:21.658813Z","caller":"traceutil/trace.go:171","msg":"trace[900160024] linearizableReadLoop","detail":"{readStateIndex:29849; appliedIndex:29849; }","duration":"214.053471ms","start":"2024-03-20T05:42:21.444732Z","end":"2024-03-20T05:42:21.658785Z","steps":["trace[900160024] 'read index received'  (duration: 214.04497ms)","trace[900160024] 'applied index is now lower than readState.Index'  (duration: 7.1¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-03-20T05:42:21.659175Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"214.389283ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-03-20T05:42:21.659229Z","caller":"traceutil/trace.go:171","msg":"trace[2047001867] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:23929; }","duration":"214.516588ms","start":"2024-03-20T05:42:21.444695Z","end":"2024-03-20T05:42:21.659212Z","steps":["trace[2047001867] 'agreement among raft nodes before linearized reading'  (duration: 214.359482ms)"],"step_count":1}
{"level":"info","ts":"2024-03-20T05:43:12.831417Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23732}
{"level":"info","ts":"2024-03-20T05:43:12.834076Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":23732,"took":"2.252716ms","hash":630350669}
{"level":"info","ts":"2024-03-20T05:43:12.834147Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":630350669,"revision":23732,"compact-revision":23492}
{"level":"info","ts":"2024-03-20T05:44:54.41072Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":30003,"local-member-snapshot-index":20002,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-03-20T05:44:54.428604Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":30003}
{"level":"info","ts":"2024-03-20T05:44:54.428817Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":25003}
{"level":"info","ts":"2024-03-20T05:46:26.64969Z","caller":"traceutil/trace.go:171","msg":"trace[1738566673] transaction","detail":"{read_only:false; response_revision:24126; number_of_response:1; }","duration":"102.584679ms","start":"2024-03-20T05:46:26.547074Z","end":"2024-03-20T05:46:26.649659Z","steps":["trace[1738566673] 'process raft request'  (duration: 11.753328ms)","trace[1738566673] 'compare'  (duration: 90.404153ms)"],"step_count":2}
{"level":"warn","ts":"2024-03-20T05:46:26.64993Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.558585ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" ","response":"range_response_count:1 size:480"}
{"level":"info","ts":"2024-03-20T05:46:26.650028Z","caller":"traceutil/trace.go:171","msg":"trace[1707568804] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:24126; }","duration":"101.682285ms","start":"2024-03-20T05:46:26.548322Z","end":"2024-03-20T05:46:26.650004Z","steps":["trace[1707568804] 'agreement among raft nodes before linearized reading'  (duration: 101.475686ms)"],"step_count":1}
{"level":"info","ts":"2024-03-20T05:46:26.649593Z","caller":"traceutil/trace.go:171","msg":"trace[439075044] linearizableReadLoop","detail":"{readStateIndex:30096; appliedIndex:30095; }","duration":"101.163687ms","start":"2024-03-20T05:46:26.548384Z","end":"2024-03-20T05:46:26.649547Z","steps":["trace[439075044] 'read index received'  (duration: 10.489136ms)","trace[439075044] 'applied index is now lower than readState.Index'  (duration: 90.672251ms)"],"step_count":2}
{"level":"info","ts":"2024-03-20T05:48:12.848198Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23971}
{"level":"info","ts":"2024-03-20T05:48:12.850474Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":23971,"took":"1.959498ms","hash":1908291741}
{"level":"info","ts":"2024-03-20T05:48:12.850609Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1908291741,"revision":23971,"compact-revision":23732}
{"level":"warn","ts":"2024-03-20T05:49:26.659741Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.422586ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128027944966191690 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:24261 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128027944966191688 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2024-03-20T05:49:26.661213Z","caller":"traceutil/trace.go:171","msg":"trace[1412353517] transaction","detail":"{read_only:false; response_revision:24269; number_of_response:1; }","duration":"114.566086ms","start":"2024-03-20T05:49:26.546598Z","end":"2024-03-20T05:49:26.661164Z","steps":["trace[1412353517] 'process raft request'  (duration: 11.887415ms)","trace[1412353517] 'compare'  (duration: 100.160388ms)"],"step_count":2}
{"level":"info","ts":"2024-03-20T05:51:26.1586Z","caller":"traceutil/trace.go:171","msg":"trace[1962887727] transaction","detail":"{read_only:false; response_revision:24364; number_of_response:1; }","duration":"102.467899ms","start":"2024-03-20T05:51:26.056084Z","end":"2024-03-20T05:51:26.158552Z","steps":["trace[1962887727] 'process raft request'  (duration: 102.195699ms)"],"step_count":1}
{"level":"info","ts":"2024-03-20T05:51:26.66567Z","caller":"traceutil/trace.go:171","msg":"trace[2026115827] transaction","detail":"{read_only:false; response_revision:24365; number_of_response:1; }","duration":"120.806334ms","start":"2024-03-20T05:51:26.544812Z","end":"2024-03-20T05:51:26.665618Z","steps":["trace[2026115827] 'process raft request'  (duration: 98.370991ms)","trace[2026115827] 'compare'  (duration: 21.587742ms)"],"step_count":2}
{"level":"info","ts":"2024-03-20T05:53:12.860245Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24211}
{"level":"info","ts":"2024-03-20T05:53:12.86249Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":24211,"took":"1.659927ms","hash":2123865703}
{"level":"info","ts":"2024-03-20T05:53:12.862616Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2123865703,"revision":24211,"compact-revision":23971}
{"level":"info","ts":"2024-03-20T05:54:23.651386Z","caller":"traceutil/trace.go:171","msg":"trace[1953346197] transaction","detail":"{read_only:false; response_revision:24504; number_of_response:1; }","duration":"100.253414ms","start":"2024-03-20T05:54:23.550973Z","end":"2024-03-20T05:54:23.651227Z","steps":["trace[1953346197] 'process raft request'  (duration: 99.840614ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-20T05:54:46.649293Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"388.807687ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-03-20T05:54:46.64981Z","caller":"traceutil/trace.go:171","msg":"trace[1399685039] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:24521; }","duration":"393.846241ms","start":"2024-03-20T05:54:46.255683Z","end":"2024-03-20T05:54:46.649529Z","steps":["trace[1399685039] 'agreement among raft nodes before linearized reading'  (duration: 97.790716ms)","trace[1399685039] 'range keys from in-memory index tree'  (duration: 289.797381ms)"],"step_count":2}
{"level":"warn","ts":"2024-03-20T05:54:46.650201Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-20T05:54:46.255661Z","time spent":"394.325037ms","remote":"127.0.0.1:35612","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-03-20T05:54:47.061304Z","caller":"traceutil/trace.go:171","msg":"trace[1303552764] transaction","detail":"{read_only:false; response_revision:24522; number_of_response:1; }","duration":"104.618455ms","start":"2024-03-20T05:54:46.956606Z","end":"2024-03-20T05:54:47.061225Z","steps":["trace[1303552764] 'process raft request'  (duration: 102.93087ms)"],"step_count":1}
{"level":"info","ts":"2024-03-20T05:54:59.042049Z","caller":"traceutil/trace.go:171","msg":"trace[708623012] transaction","detail":"{read_only:false; response_revision:24532; number_of_response:1; }","duration":"191.315835ms","start":"2024-03-20T05:54:58.850672Z","end":"2024-03-20T05:54:59.041988Z","steps":["trace[708623012] 'process raft request'  (duration: 191.037637ms)"],"step_count":1}
{"level":"info","ts":"2024-03-20T05:58:12.872398Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24448}
{"level":"info","ts":"2024-03-20T05:58:12.876064Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":24448,"took":"3.010466ms","hash":1526355495}
{"level":"info","ts":"2024-03-20T05:58:12.876308Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1526355495,"revision":24448,"compact-revision":24211}
{"level":"info","ts":"2024-03-20T05:58:36.251909Z","caller":"traceutil/trace.go:171","msg":"trace[1923162782] transaction","detail":"{read_only:false; response_revision:24705; number_of_response:1; }","duration":"109.11264ms","start":"2024-03-20T05:58:36.142679Z","end":"2024-03-20T05:58:36.251791Z","steps":["trace[1923162782] 'process raft request'  (duration: 11.884961ms)","trace[1923162782] 'compare'  (duration: 96.401982ms)"],"step_count":2}
{"level":"info","ts":"2024-03-20T06:00:03.551367Z","caller":"traceutil/trace.go:171","msg":"trace[246926116] transaction","detail":"{read_only:false; response_revision:24773; number_of_response:1; }","duration":"100.417353ms","start":"2024-03-20T06:00:03.450916Z","end":"2024-03-20T06:00:03.551333Z","steps":["trace[246926116] 'process raft request'  (duration: 99.723848ms)"],"step_count":1}
{"level":"info","ts":"2024-03-20T06:01:19.440215Z","caller":"traceutil/trace.go:171","msg":"trace[1304758700] transaction","detail":"{read_only:false; response_revision:24835; number_of_response:1; }","duration":"100.764566ms","start":"2024-03-20T06:01:19.33938Z","end":"2024-03-20T06:01:19.440144Z","steps":["trace[1304758700] 'process raft request'  (duration: 100.162066ms)"],"step_count":1}
{"level":"info","ts":"2024-03-20T06:03:12.880281Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24685}
{"level":"info","ts":"2024-03-20T06:03:12.881815Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":24685,"took":"1.258419ms","hash":4251245100}
{"level":"info","ts":"2024-03-20T06:03:12.881908Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4251245100,"revision":24685,"compact-revision":24448}

* 
* ==> etcd [fde85ec9ec67] <==
* {"level":"info","ts":"2024-03-21T18:44:40.531421Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2163528240,"revision":30395,"compact-revision":30153}
{"level":"info","ts":"2024-03-21T18:49:40.539447Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":30635}
{"level":"info","ts":"2024-03-21T18:49:40.540531Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":30635,"took":"823.202¬µs","hash":2572106138}
{"level":"info","ts":"2024-03-21T18:49:40.540592Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2572106138,"revision":30635,"compact-revision":30395}
{"level":"info","ts":"2024-03-21T18:54:40.550577Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":30932}
{"level":"info","ts":"2024-03-21T18:54:40.552296Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":30932,"took":"1.259435ms","hash":3012510164}
{"level":"info","ts":"2024-03-21T18:54:40.552368Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3012510164,"revision":30932,"compact-revision":30635}
{"level":"info","ts":"2024-03-21T19:09:56.280788Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":31172}
{"level":"info","ts":"2024-03-21T19:09:56.284302Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":31172,"took":"2.760093ms","hash":3793983470}
{"level":"info","ts":"2024-03-21T19:09:56.284442Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3793983470,"revision":31172,"compact-revision":30932}
{"level":"info","ts":"2024-03-21T19:10:44.923011Z","caller":"traceutil/trace.go:171","msg":"trace[631841077] transaction","detail":"{read_only:false; response_revision:31451; number_of_response:1; }","duration":"100.776315ms","start":"2024-03-21T19:10:44.822183Z","end":"2024-03-21T19:10:44.92296Z","steps":["trace[631841077] 'process raft request'  (duration: 100.518415ms)"],"step_count":1}
{"level":"info","ts":"2024-03-21T19:12:10.012263Z","caller":"traceutil/trace.go:171","msg":"trace[164619505] transaction","detail":"{read_only:false; response_revision:31519; number_of_response:1; }","duration":"177.242839ms","start":"2024-03-21T19:12:09.834959Z","end":"2024-03-21T19:12:10.012202Z","steps":["trace[164619505] 'process raft request'  (duration: 176.841648ms)"],"step_count":1}
{"level":"info","ts":"2024-03-21T19:16:57.844904Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":31412}
{"level":"info","ts":"2024-03-21T19:16:57.848284Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":31412,"took":"2.072998ms","hash":3771709133}
{"level":"info","ts":"2024-03-21T19:16:57.848416Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3771709133,"revision":31412,"compact-revision":31172}
{"level":"info","ts":"2024-03-21T19:22:55.714579Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":31651}
{"level":"info","ts":"2024-03-21T19:22:55.715324Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":31651,"took":"548.8¬µs","hash":646263305}
{"level":"info","ts":"2024-03-21T19:22:55.715363Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":646263305,"revision":31651,"compact-revision":31412}
{"level":"info","ts":"2024-03-21T19:27:52.259984Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":40004,"local-member-snapshot-index":30003,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-03-21T19:27:52.272375Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":40004}
{"level":"info","ts":"2024-03-21T19:27:52.27251Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":35004}
{"level":"info","ts":"2024-03-21T19:27:55.722046Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":31891}
{"level":"info","ts":"2024-03-21T19:27:55.722904Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":31891,"took":"621.799¬µs","hash":3632077376}
{"level":"info","ts":"2024-03-21T19:27:55.722949Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3632077376,"revision":31891,"compact-revision":31651}
{"level":"info","ts":"2024-03-21T19:32:55.729229Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32132}
{"level":"info","ts":"2024-03-21T19:32:55.730382Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":32132,"took":"918.603¬µs","hash":2666995097}
{"level":"info","ts":"2024-03-21T19:32:55.73043Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2666995097,"revision":32132,"compact-revision":31891}
{"level":"info","ts":"2024-03-21T19:37:55.737252Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32372}
{"level":"info","ts":"2024-03-21T19:37:55.738179Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":32372,"took":"652.2¬µs","hash":2423178592}
{"level":"info","ts":"2024-03-21T19:37:55.738225Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2423178592,"revision":32372,"compact-revision":32132}
{"level":"info","ts":"2024-03-21T19:42:55.745906Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32612}
{"level":"info","ts":"2024-03-21T19:42:55.746845Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":32612,"took":"656.605¬µs","hash":4125495709}
{"level":"info","ts":"2024-03-21T19:42:55.746892Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4125495709,"revision":32612,"compact-revision":32372}
{"level":"info","ts":"2024-03-21T19:47:55.753739Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32852}
{"level":"info","ts":"2024-03-21T19:47:55.754383Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":32852,"took":"480.602¬µs","hash":1772858761}
{"level":"info","ts":"2024-03-21T19:47:55.754421Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1772858761,"revision":32852,"compact-revision":32612}
{"level":"info","ts":"2024-03-21T19:52:55.761552Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33092}
{"level":"info","ts":"2024-03-21T19:52:55.762434Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":33092,"took":"660.4¬µs","hash":2495543163}
{"level":"info","ts":"2024-03-21T19:52:55.76248Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2495543163,"revision":33092,"compact-revision":32852}
{"level":"info","ts":"2024-03-21T19:57:55.770384Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33332}
{"level":"info","ts":"2024-03-21T19:57:55.771914Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":33332,"took":"1.192707ms","hash":3125926335}
{"level":"info","ts":"2024-03-21T19:57:55.771985Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3125926335,"revision":33332,"compact-revision":33092}
{"level":"warn","ts":"2024-03-21T19:59:35.131415Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.978045ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csinodes/\" range_end:\"/registry/csinodes0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-03-21T19:59:35.132276Z","caller":"traceutil/trace.go:171","msg":"trace[530075306] range","detail":"{range_begin:/registry/csinodes/; range_end:/registry/csinodes0; response_count:0; response_revision:33651; }","duration":"117.52624ms","start":"2024-03-21T19:59:35.014255Z","end":"2024-03-21T19:59:35.131781Z","steps":["trace[530075306] 'count revisions from in-memory index tree'  (duration: 113.719145ms)"],"step_count":1}
{"level":"info","ts":"2024-03-21T20:02:55.778868Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33572}
{"level":"info","ts":"2024-03-21T20:02:55.77997Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":33572,"took":"868.202¬µs","hash":878665064}
{"level":"info","ts":"2024-03-21T20:02:55.780006Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":878665064,"revision":33572,"compact-revision":33332}
{"level":"info","ts":"2024-03-21T20:07:55.787354Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33814}
{"level":"info","ts":"2024-03-21T20:07:55.788336Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":33814,"took":"755.9¬µs","hash":692969811}
{"level":"info","ts":"2024-03-21T20:07:55.788385Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":692969811,"revision":33814,"compact-revision":33572}
{"level":"info","ts":"2024-03-21T20:12:55.796148Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":34054}
{"level":"info","ts":"2024-03-21T20:12:55.796902Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":34054,"took":"570¬µs","hash":3134502025}
{"level":"info","ts":"2024-03-21T20:12:55.796943Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3134502025,"revision":34054,"compact-revision":33814}
{"level":"info","ts":"2024-03-21T20:17:55.803697Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":34295}
{"level":"info","ts":"2024-03-21T20:17:55.805218Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":34295,"took":"1.20151ms","hash":2893683554}
{"level":"info","ts":"2024-03-21T20:17:55.805285Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2893683554,"revision":34295,"compact-revision":34054}
{"level":"info","ts":"2024-03-21T20:20:14.013567Z","caller":"traceutil/trace.go:171","msg":"trace[123442101] transaction","detail":"{read_only:false; response_revision:34645; number_of_response:1; }","duration":"100.030377ms","start":"2024-03-21T20:20:13.910805Z","end":"2024-03-21T20:20:14.010835Z","steps":["trace[123442101] 'process raft request'  (duration: 99.893576ms)"],"step_count":1}
{"level":"info","ts":"2024-03-21T20:22:55.811591Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":34534}
{"level":"info","ts":"2024-03-21T20:22:55.812702Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":34534,"took":"853.905¬µs","hash":3093867754}
{"level":"info","ts":"2024-03-21T20:22:55.812755Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3093867754,"revision":34534,"compact-revision":34295}

* 
* ==> kernel <==
*  20:27:46 up  3:16,  0 users,  load average: 0.06, 0.18, 0.24
Linux minikube 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [1b85d845b6f0] <==
* I0321 16:59:43.902534       1 aggregator.go:164] waiting for initial CRD sync...
I0321 16:59:43.902573       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0321 16:59:43.902586       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0321 16:59:43.903055       1 controller.go:134] Starting OpenAPI controller
I0321 16:59:43.901978       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0321 16:59:43.904380       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0321 16:59:43.904458       1 controller.go:85] Starting OpenAPI V3 controller
I0321 16:59:43.904496       1 naming_controller.go:291] Starting NamingConditionController
I0321 16:59:43.904542       1 establishing_controller.go:76] Starting EstablishingController
I0321 16:59:43.904586       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0321 16:59:43.904622       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I0321 16:59:43.904731       1 crd_finalizer.go:266] Starting CRDFinalizer
I0321 16:59:43.908808       1 available_controller.go:423] Starting AvailableConditionController
I0321 16:59:43.908853       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0321 16:59:44.260181       1 shared_informer.go:318] Caches are synced for configmaps
I0321 16:59:44.261312       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0321 16:59:44.261493       1 aggregator.go:166] initial CRD sync complete...
I0321 16:59:44.261670       1 autoregister_controller.go:141] Starting autoregister controller
I0321 16:59:44.261843       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0321 16:59:44.277057       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0321 16:59:44.360286       1 shared_informer.go:318] Caches are synced for node_authorizer
I0321 16:59:44.360398       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0321 16:59:44.360488       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0321 16:59:44.360592       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0321 16:59:44.361140       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0321 16:59:44.362024       1 cache.go:39] Caches are synced for autoregister controller
I0321 16:59:44.362790       1 cache.go:39] Caches are synced for AvailableConditionController controller
E0321 16:59:44.460453       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0321 16:59:44.967508       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0321 16:59:52.573454       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0321 16:59:52.587284       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0321 16:59:52.688253       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0321 16:59:52.885648       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0321 16:59:52.911449       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0321 17:00:03.872153       1 controller.go:624] quota admission added evaluator for: endpoints
I0321 17:00:03.968968       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0321 17:03:06.561671       1 trace.go:236] Trace[492734209]: "Update" accept:application/json, */*,audit-id:02a5b280-89e8-4b01-acc3-40c5c25e7f2c,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (21-Mar-2024 17:03:05.388) (total time: 1173ms):
Trace[492734209]: ---"Conversion done" 29ms (17:03:05.418)
Trace[492734209]: ["GuaranteedUpdate etcd3" audit-id:02a5b280-89e8-4b01-acc3-40c5c25e7f2c,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1143ms (17:03:05.418)
Trace[492734209]:  ---"Txn call completed" 1142ms (17:03:06.561)]
Trace[492734209]: [1.173099245s] [1.173099245s] END
I0321 17:09:31.266416       1 alloc.go:330] "allocated clusterIPs" service="default/k8s-web-hello-deploy" clusterIPs={"IPv4":"10.97.157.63"}
I0321 17:25:13.921926       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0321 17:33:23.956146       1 controller.go:624] quota admission added evaluator for: namespaces
I0321 17:33:24.412171       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs={"IPv4":"10.108.201.97"}
I0321 17:33:24.440478       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs={"IPv4":"10.109.4.77"}
I0321 17:45:39.468873       1 trace.go:236] Trace[478654343]: "Delete" accept:application/json,audit-id:1bdc2b8c-fe33-4301-ad8b-15f833c6db56,client:192.168.49.1,protocol:HTTP/2.0,resource:services,scope:resource,url:/api/v1/namespaces/default/services/k8s-web-hello-deploy,user-agent:kubectl.exe/v1.26.14 (windows/amd64) kubernetes/929bbaf,verb:DELETE (21-Mar-2024 17:45:38.663) (total time: 805ms):
Trace[478654343]: ---"Object deleted from database" 804ms (17:45:39.467)
Trace[478654343]: [805.131077ms] [805.131077ms] END
I0321 17:45:41.387229       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0321 17:45:41.564547       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0321 18:03:13.711045       1 trace.go:236] Trace[1553211890]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:614d94cf-59c1-47c0-98b6-2fa59cd0620a,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (21-Mar-2024 18:03:12.223) (total time: 832ms):
Trace[1553211890]: ["GuaranteedUpdate etcd3" audit-id:614d94cf-59c1-47c0-98b6-2fa59cd0620a,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 1535ms (18:03:12.223)
Trace[1553211890]:  ---"Txn call completed" 798ms (18:03:13.056)]
Trace[1553211890]: [832.967727ms] [832.967727ms] END
I0321 18:33:20.403951       1 trace.go:236] Trace[188040270]: "Update" accept:application/json, */*,audit-id:795b9326-f628-4b06-ae8a-defc157f268b,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (21-Mar-2024 18:33:19.657) (total time: 746ms):
Trace[188040270]: ["GuaranteedUpdate etcd3" audit-id:795b9326-f628-4b06-ae8a-defc157f268b,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 745ms (18:33:19.658)
Trace[188040270]:  ---"Txn call completed" 713ms (18:33:20.403)]
Trace[188040270]: [746.185342ms] [746.185342ms] END
I0321 20:25:40.637147       1 alloc.go:330] "allocated clusterIPs" service="default/k8s-web-hello-svc" clusterIPs={"IPv4":"10.111.0.47"}

* 
* ==> kube-apiserver [6ad430f1359c] <==
* I0320 04:13:15.368337       1 autoregister_controller.go:141] Starting autoregister controller
I0320 04:13:15.368364       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0320 04:13:15.368379       1 cache.go:39] Caches are synced for autoregister controller
I0320 04:13:15.370917       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0320 04:13:15.370953       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0320 04:13:15.371299       1 shared_informer.go:318] Caches are synced for node_authorizer
I0320 04:13:15.371607       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
E0320 04:13:15.388470       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0320 04:13:15.464830       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0320 04:13:16.011079       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0320 04:13:19.566661       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0320 04:13:19.667582       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0320 04:13:19.786197       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0320 04:13:19.899999       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0320 04:13:19.971745       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0320 04:13:24.977251       1 controller.go:624] quota admission added evaluator for: endpoints
I0320 04:13:29.480796       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0320 04:48:51.903298       1 trace.go:236] Trace[1513250309]: "Get" accept:application/json, */*,audit-id:71e1d6bf-6f0d-47db-9bab-403e6d637807,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (20-Mar-2024 04:48:50.392) (total time: 1501ms):
Trace[1513250309]: ---"About to write a response" 1500ms (04:48:51.893)
Trace[1513250309]: [1.501076226s] [1.501076226s] END
I0320 04:49:05.884664       1 trace.go:236] Trace[730164081]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:ee29f7c1-5e90-4c84-8b51-1a71f5e854bf,client:192.168.49.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube/status,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (20-Mar-2024 04:49:05.298) (total time: 585ms):
Trace[730164081]: ["GuaranteedUpdate etcd3" audit-id:ee29f7c1-5e90-4c84-8b51-1a71f5e854bf,key:/minions/minikube,type:*core.Node,resource:nodes 585ms (04:49:05.299)
Trace[730164081]:  ---"About to Encode" 182ms (04:49:05.486)
Trace[730164081]:  ---"Txn call completed" 387ms (04:49:05.874)]
Trace[730164081]: ---"Object stored in database" 575ms (04:49:05.883)
Trace[730164081]: [585.6512ms] [585.6512ms] END
I0320 04:49:06.476164       1 trace.go:236] Trace[188760372]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (20-Mar-2024 04:49:05.886) (total time: 509ms):
Trace[188760372]: ---"initial value restored" 187ms (04:49:06.074)
Trace[188760372]: ---"Transaction prepared" 100ms (04:49:06.174)
Trace[188760372]: ---"Txn call completed" 222ms (04:49:06.396)
Trace[188760372]: [509.842503ms] [509.842503ms] END
I0320 05:12:16.074964       1 trace.go:236] Trace[1271193921]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:0800d0a9-1b10-443e-b511-d50a564db949,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (20-Mar-2024 05:12:15.566) (total time: 508ms):
Trace[1271193921]: ["GuaranteedUpdate etcd3" audit-id:0800d0a9-1b10-443e-b511-d50a564db949,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 507ms (05:12:15.566)
Trace[1271193921]:  ---"About to Encode" 399ms (05:12:15.966)
Trace[1271193921]:  ---"Txn call completed" 108ms (05:12:16.074)]
Trace[1271193921]: [508.084184ms] [508.084184ms] END
I0320 05:25:13.555276       1 trace.go:236] Trace[411084682]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:6b9bca19-5600-4174-aa76-d82a76f50983,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (20-Mar-2024 05:25:11.026) (total time: 2528ms):
Trace[411084682]: ["GuaranteedUpdate etcd3" audit-id:6b9bca19-5600-4174-aa76-d82a76f50983,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 2528ms (05:25:11.027)
Trace[411084682]:  ---"About to Encode" 394ms (05:25:11.421)
Trace[411084682]:  ---"Txn call completed" 2133ms (05:25:13.554)]
Trace[411084682]: [2.528349398s] [2.528349398s] END
I0320 05:25:16.831722       1 trace.go:236] Trace[557743990]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:d714b23b-69b6-44d4-b431-4169076a74b2,client:127.0.0.1,protocol:HTTP/2.0,resource:services,scope:cluster,url:/api/v1/services,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:LIST (20-Mar-2024 05:25:15.922) (total time: 907ms):
Trace[557743990]: ["List(recursive=true) etcd3" audit-id:d714b23b-69b6-44d4-b431-4169076a74b2,key:/services/specs,resourceVersion:,resourceVersionMatch:,limit:0,continue: 908ms (05:25:15.922)]
Trace[557743990]: [907.770057ms] [907.770057ms] END
I0320 05:25:17.828006       1 trace.go:236] Trace[1213807585]: "GuaranteedUpdate etcd3" audit-id:,key:/ranges/serviceips,type:*core.RangeAllocation,resource:serviceipallocations (20-Mar-2024 05:25:17.222) (total time: 605ms):
Trace[1213807585]: ---"initial value restored" 604ms (05:25:17.827)
Trace[1213807585]: [605.379007ms] [605.379007ms] END
I0320 05:25:17.832120       1 trace.go:236] Trace[117547646]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (20-Mar-2024 05:25:16.221) (total time: 1610ms):
Trace[117547646]: ---"initial value restored" 611ms (05:25:16.832)
Trace[117547646]: ---"Transaction prepared" 293ms (05:25:17.125)
Trace[117547646]: ---"Txn call completed" 705ms (05:25:17.831)
Trace[117547646]: [1.61060855s] [1.61060855s] END
I0320 05:25:17.933516       1 trace.go:236] Trace[759756121]: "Get" accept:application/json, */*,audit-id:10bfc039-ec01-4453-82d0-1f130e8c991f,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (20-Mar-2024 05:25:17.421) (total time: 511ms):
Trace[759756121]: ---"About to write a response" 511ms (05:25:17.933)
Trace[759756121]: [511.981257ms] [511.981257ms] END
I0320 05:54:47.155797       1 trace.go:236] Trace[1337239912]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (20-Mar-2024 05:54:46.542) (total time: 600ms):
Trace[1337239912]: ---"initial value restored" 101ms (05:54:46.643)
Trace[1337239912]: ---"Transaction prepared" 197ms (05:54:46.841)
Trace[1337239912]: ---"Txn call completed" 300ms (05:54:47.142)
Trace[1337239912]: [600.499474ms] [600.499474ms] END

* 
* ==> kube-controller-manager [10771fe52a34] <==
* I0321 17:33:24.167003       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-8694d4445c-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0321 17:33:24.178261       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="11.622232ms"
E0321 17:33:24.178326       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" failed with pods "dashboard-metrics-scraper-7fd5cb4ddc-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0321 17:33:24.178325       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-7fd5cb4ddc-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0321 17:33:24.181907       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="13.78352ms"
E0321 17:33:24.182003       1 replica_set.go:557] sync "kubernetes-dashboard/kubernetes-dashboard-8694d4445c" failed with pods "kubernetes-dashboard-8694d4445c-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0321 17:33:24.181932       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-8694d4445c-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0321 17:33:24.202475       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kubernetes-dashboard-8694d4445c-dhrhq"
I0321 17:33:24.213676       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="24.636857ms"
I0321 17:33:24.218169       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dashboard-metrics-scraper-7fd5cb4ddc-jzsgf"
I0321 17:33:24.275586       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="61.805042ms"
I0321 17:33:24.275780       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="67.599¬µs"
I0321 17:33:24.281091       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="74.09517ms"
I0321 17:33:24.287640       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="72.2¬µs"
I0321 17:33:24.297035       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="15.793408ms"
I0321 17:33:24.297228       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="60.8¬µs"
I0321 17:33:24.377459       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="107.1¬µs"
I0321 17:33:41.566630       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="76.201723ms"
I0321 17:33:41.566895       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="106.3¬µs"
I0321 17:33:45.524119       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="11.019601ms"
I0321 17:33:45.524342       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="113.7¬µs"
I0321 17:45:38.171494       1 event.go:307] "Event occurred" object="default/k8s-web-hello-deploy-7bb449b5d8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: k8s-web-hello-deploy-7bb449b5d8-phss9"
I0321 17:45:38.271495       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-deploy-7bb449b5d8" duration="141.949332ms"
I0321 17:45:38.366401       1 event.go:307] "Event occurred" object="default/k8s-web-hello-deploy-7bb449b5d8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: k8s-web-hello-deploy-7bb449b5d8-dzwnv"
I0321 17:45:38.477832       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-deploy-7bb449b5d8" duration="206.16891ms"
I0321 17:45:38.564264       1 event.go:307] "Event occurred" object="default/k8s-web-hello-deploy-7bb449b5d8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: k8s-web-hello-deploy-7bb449b5d8-bv8qh"
I0321 17:45:38.662891       1 event.go:307] "Event occurred" object="default/k8s-web-hello-deploy-7bb449b5d8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: k8s-web-hello-deploy-7bb449b5d8-t84gk"
I0321 17:45:38.684469       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-deploy-7bb449b5d8" duration="206.459409ms"
I0321 17:45:38.772566       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-deploy-7bb449b5d8" duration="88.000034ms"
I0321 17:45:38.773044       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-deploy-7bb449b5d8" duration="271.4¬µs"
I0321 17:45:38.963847       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-deploy-7bb449b5d8" duration="111.1¬µs"
I0321 17:45:39.376948       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-deploy-7bb449b5d8" duration="182.999¬µs"
I0321 17:45:40.074767       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-deploy-64846d8668" duration="17.2¬µs"
I0321 17:45:40.078287       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-deploy-7bb449b5d8" duration="16.2¬µs"
I0321 18:12:01.234974       1 event.go:307] "Event occurred" object="default/k8s-web-hello" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set k8s-web-hello-f6bcdcc9c to 1"
I0321 18:12:01.260904       1 event.go:307] "Event occurred" object="default/k8s-web-hello-f6bcdcc9c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: k8s-web-hello-f6bcdcc9c-mqxcr"
I0321 18:12:01.270397       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="36.182971ms"
I0321 18:12:01.331802       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="61.30535ms"
I0321 18:12:01.331972       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="88¬µs"
I0321 18:12:01.332065       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="37.599¬µs"
I0321 18:12:03.295470       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="8.086917ms"
I0321 18:12:03.295585       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="31.3¬µs"
I0321 18:47:47.193603       1 event.go:307] "Event occurred" object="default/k8s-web-hello" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set k8s-web-hello-f6bcdcc9c to 5 from 1"
I0321 18:47:47.209838       1 event.go:307] "Event occurred" object="default/k8s-web-hello-f6bcdcc9c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: k8s-web-hello-f6bcdcc9c-spjw6"
I0321 18:47:47.219498       1 event.go:307] "Event occurred" object="default/k8s-web-hello-f6bcdcc9c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: k8s-web-hello-f6bcdcc9c-cbzwh"
I0321 18:47:47.219574       1 event.go:307] "Event occurred" object="default/k8s-web-hello-f6bcdcc9c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: k8s-web-hello-f6bcdcc9c-vfqqt"
I0321 18:47:47.230325       1 event.go:307] "Event occurred" object="default/k8s-web-hello-f6bcdcc9c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: k8s-web-hello-f6bcdcc9c-c6b9t"
I0321 18:47:47.302167       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="109.126877ms"
I0321 18:47:47.320212       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="17.984613ms"
I0321 18:47:47.320355       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="46.3¬µs"
I0321 18:47:47.333481       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="98.8¬µs"
I0321 18:47:47.343498       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="38.6¬µs"
I0321 18:47:49.510071       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="63.496945ms"
I0321 18:47:49.510249       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="91.8¬µs"
I0321 18:47:50.818386       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="7.427113ms"
I0321 18:47:50.818524       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="69.7¬µs"
I0321 18:47:51.761710       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="7.348413ms"
I0321 18:47:51.761870       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="62.5¬µs"
I0321 18:47:51.811322       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="47.079383ms"
I0321 18:47:51.811468       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-f6bcdcc9c" duration="64.5¬µs"

* 
* ==> kube-controller-manager [fa6a0ae557c8] <==
* I0320 04:13:29.130786       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0320 04:13:29.130812       1 resource_quota_monitor.go:305] "QuotaMonitor running"
I0320 04:13:29.173965       1 controllermanager.go:642] "Started controller" controller="serviceaccount-controller"
I0320 04:13:29.174125       1 serviceaccounts_controller.go:111] "Starting service account controller"
I0320 04:13:29.174141       1 shared_informer.go:311] Waiting for caches to sync for service account
I0320 04:13:29.221010       1 controllermanager.go:642] "Started controller" controller="bootstrap-signer-controller"
I0320 04:13:29.221222       1 shared_informer.go:311] Waiting for caches to sync for bootstrap_signer
I0320 04:13:29.226484       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0320 04:13:29.232221       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0320 04:13:29.269624       1 shared_informer.go:318] Caches are synced for cronjob
I0320 04:13:29.271341       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0320 04:13:29.274843       1 shared_informer.go:318] Caches are synced for deployment
I0320 04:13:29.274902       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0320 04:13:29.277957       1 shared_informer.go:318] Caches are synced for TTL after finished
I0320 04:13:29.279288       1 shared_informer.go:318] Caches are synced for disruption
I0320 04:13:29.288134       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0320 04:13:29.297606       1 shared_informer.go:318] Caches are synced for node
I0320 04:13:29.297704       1 range_allocator.go:174] "Sending events to api server"
I0320 04:13:29.297737       1 range_allocator.go:178] "Starting range CIDR allocator"
I0320 04:13:29.297747       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0320 04:13:29.297758       1 shared_informer.go:318] Caches are synced for cidrallocator
I0320 04:13:29.302862       1 shared_informer.go:318] Caches are synced for attach detach
I0320 04:13:29.306236       1 shared_informer.go:318] Caches are synced for crt configmap
I0320 04:13:29.364685       1 shared_informer.go:318] Caches are synced for HPA
I0320 04:13:29.364708       1 shared_informer.go:318] Caches are synced for persistent volume
I0320 04:13:29.364782       1 shared_informer.go:318] Caches are synced for job
I0320 04:13:29.365050       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0320 04:13:29.365060       1 shared_informer.go:318] Caches are synced for expand
I0320 04:13:29.365111       1 shared_informer.go:318] Caches are synced for PVC protection
I0320 04:13:29.365116       1 shared_informer.go:318] Caches are synced for PV protection
I0320 04:13:29.365219       1 shared_informer.go:318] Caches are synced for TTL
I0320 04:13:29.365265       1 shared_informer.go:318] Caches are synced for endpoint
I0320 04:13:29.365272       1 shared_informer.go:318] Caches are synced for ephemeral
I0320 04:13:29.365290       1 shared_informer.go:318] Caches are synced for GC
I0320 04:13:29.365319       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0320 04:13:29.365758       1 shared_informer.go:318] Caches are synced for stateful set
I0320 04:13:29.371151       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0320 04:13:29.371739       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8s-web-hello-deploy-7bb449b5d8" duration="198.8¬µs"
I0320 04:13:29.371744       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="226.7¬µs"
I0320 04:13:29.371831       1 shared_informer.go:318] Caches are synced for ReplicationController
I0320 04:13:29.381953       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0320 04:13:29.383208       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0320 04:13:29.464794       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0320 04:13:29.464878       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0320 04:13:29.465011       1 shared_informer.go:318] Caches are synced for namespace
I0320 04:13:29.466365       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0320 04:13:29.474449       1 shared_informer.go:318] Caches are synced for service account
I0320 04:13:29.486586       1 shared_informer.go:318] Caches are synced for daemon sets
I0320 04:13:29.564762       1 shared_informer.go:318] Caches are synced for resource quota
I0320 04:13:29.564896       1 shared_informer.go:318] Caches are synced for resource quota
I0320 04:13:29.564775       1 shared_informer.go:318] Caches are synced for taint
I0320 04:13:29.565141       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0320 04:13:29.565412       1 taint_manager.go:211] "Sending events to api server"
I0320 04:13:29.565723       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0320 04:13:29.566423       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0320 04:13:29.566440       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0320 04:13:29.566795       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0320 04:13:29.872169       1 shared_informer.go:318] Caches are synced for garbage collector
I0320 04:13:29.964779       1 shared_informer.go:318] Caches are synced for garbage collector
I0320 04:13:29.964819       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"

* 
* ==> kube-proxy [4be85200df3d] <==
* I0320 04:13:18.892455       1 server_others.go:69] "Using iptables proxy"
I0320 04:13:19.076560       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0320 04:13:19.367036       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0320 04:13:19.371883       1 server_others.go:152] "Using iptables Proxier"
I0320 04:13:19.371931       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0320 04:13:19.371943       1 server_others.go:438] "Defaulting to no-op detect-local"
I0320 04:13:19.373643       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0320 04:13:19.374642       1 server.go:846] "Version info" version="v1.28.3"
I0320 04:13:19.374671       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0320 04:13:19.377415       1 config.go:97] "Starting endpoint slice config controller"
I0320 04:13:19.377636       1 config.go:315] "Starting node config controller"
I0320 04:13:19.377742       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0320 04:13:19.377756       1 shared_informer.go:311] Waiting for caches to sync for node config
I0320 04:13:19.465504       1 config.go:188] "Starting service config controller"
I0320 04:13:19.465543       1 shared_informer.go:311] Waiting for caches to sync for service config
I0320 04:13:19.567000       1 shared_informer.go:318] Caches are synced for service config
I0320 04:13:19.577894       1 shared_informer.go:318] Caches are synced for node config
I0320 04:13:19.577953       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-proxy [b5c4f1301c38] <==
* I0321 16:59:52.599285       1 server_others.go:69] "Using iptables proxy"
I0321 16:59:52.681125       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0321 16:59:53.062648       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0321 16:59:53.072212       1 server_others.go:152] "Using iptables Proxier"
I0321 16:59:53.072396       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0321 16:59:53.072423       1 server_others.go:438] "Defaulting to no-op detect-local"
I0321 16:59:53.076211       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0321 16:59:53.077397       1 server.go:846] "Version info" version="v1.28.3"
I0321 16:59:53.077518       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0321 16:59:53.082876       1 config.go:97] "Starting endpoint slice config controller"
I0321 16:59:53.084010       1 config.go:315] "Starting node config controller"
I0321 16:59:53.087809       1 shared_informer.go:311] Waiting for caches to sync for node config
I0321 16:59:53.084202       1 config.go:188] "Starting service config controller"
I0321 16:59:53.097282       1 shared_informer.go:311] Waiting for caches to sync for service config
I0321 16:59:53.087855       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0321 16:59:53.259481       1 shared_informer.go:318] Caches are synced for service config
I0321 16:59:53.260425       1 shared_informer.go:318] Caches are synced for node config
I0321 16:59:53.260678       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [71a964a2b354] <==
* I0320 04:13:12.768708       1 serving.go:348] Generated self-signed cert in-memory
W0320 04:13:15.368554       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0320 04:13:15.368976       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0320 04:13:15.369083       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0320 04:13:15.369134       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0320 04:13:15.467010       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0320 04:13:15.467147       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0320 04:13:15.471652       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0320 04:13:15.471839       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0320 04:13:15.475801       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0320 04:13:15.477492       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0320 04:13:15.573366       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [e7ca2677daf7] <==
* I0321 16:59:38.620665       1 serving.go:348] Generated self-signed cert in-memory
W0321 16:59:44.163552       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0321 16:59:44.163609       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0321 16:59:44.163637       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0321 16:59:44.163659       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0321 16:59:44.381900       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0321 16:59:44.382001       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0321 16:59:44.462760       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0321 16:59:44.466868       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0321 16:59:44.467231       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0321 16:59:44.467919       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0321 16:59:44.668273       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Mar 21 18:09:29 minikube kubelet[1693]: W0321 18:09:29.115935    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 18:12:01 minikube kubelet[1693]: I0321 18:12:01.271272    1693 topology_manager.go:215] "Topology Admit Handler" podUID="bdb386e4-ca27-4c7f-b9c9-4a9958201f9e" podNamespace="default" podName="k8s-web-hello-f6bcdcc9c-mqxcr"
Mar 21 18:12:01 minikube kubelet[1693]: E0321 18:12:01.271833    1693 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="81dc092b-d50c-403c-98f3-f21790653fef" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: E0321 18:12:01.271881    1693 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="522ac7bf-5791-482f-98f8-d0175f5517dd" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: E0321 18:12:01.271900    1693 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="84a3acee-f048-4b28-9036-dca7a2d596ea" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: E0321 18:12:01.271918    1693 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="36df047f-989f-4e51-beac-c9a2b73743c7" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: E0321 18:12:01.271931    1693 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="04f72cc4-0b86-4d3d-b512-d63236c6945b" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: E0321 18:12:01.271947    1693 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="066461da-3452-4c46-b405-e54d9036e637" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: E0321 18:12:01.271962    1693 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="e3c3623c-f424-4445-aa1e-790e399d0385" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: E0321 18:12:01.271973    1693 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="794eb809-78b9-4043-8d06-9c61bbfbcbbc" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: I0321 18:12:01.272340    1693 memory_manager.go:346] "RemoveStaleState removing state" podUID="e3c3623c-f424-4445-aa1e-790e399d0385" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: I0321 18:12:01.272358    1693 memory_manager.go:346] "RemoveStaleState removing state" podUID="84a3acee-f048-4b28-9036-dca7a2d596ea" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: I0321 18:12:01.272366    1693 memory_manager.go:346] "RemoveStaleState removing state" podUID="794eb809-78b9-4043-8d06-9c61bbfbcbbc" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: I0321 18:12:01.272375    1693 memory_manager.go:346] "RemoveStaleState removing state" podUID="066461da-3452-4c46-b405-e54d9036e637" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: I0321 18:12:01.272383    1693 memory_manager.go:346] "RemoveStaleState removing state" podUID="81dc092b-d50c-403c-98f3-f21790653fef" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: I0321 18:12:01.272391    1693 memory_manager.go:346] "RemoveStaleState removing state" podUID="04f72cc4-0b86-4d3d-b512-d63236c6945b" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: I0321 18:12:01.272399    1693 memory_manager.go:346] "RemoveStaleState removing state" podUID="36df047f-989f-4e51-beac-c9a2b73743c7" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: I0321 18:12:01.272407    1693 memory_manager.go:346] "RemoveStaleState removing state" podUID="522ac7bf-5791-482f-98f8-d0175f5517dd" containerName="k8s-web-hello"
Mar 21 18:12:01 minikube kubelet[1693]: I0321 18:12:01.312572    1693 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bdnn7\" (UniqueName: \"kubernetes.io/projected/bdb386e4-ca27-4c7f-b9c9-4a9958201f9e-kube-api-access-bdnn7\") pod \"k8s-web-hello-f6bcdcc9c-mqxcr\" (UID: \"bdb386e4-ca27-4c7f-b9c9-4a9958201f9e\") " pod="default/k8s-web-hello-f6bcdcc9c-mqxcr"
Mar 21 18:12:03 minikube kubelet[1693]: I0321 18:12:03.287868    1693 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/k8s-web-hello-f6bcdcc9c-mqxcr" podStartSLOduration=1.541694919 podCreationTimestamp="2024-03-21 18:12:01 +0000 UTC" firstStartedPulling="2024-03-21 18:12:01.876586393 +0000 UTC m=+4354.594883331" lastFinishedPulling="2024-03-21 18:12:02.622636275 +0000 UTC m=+4355.340933113" observedRunningTime="2024-03-21 18:12:03.287447796 +0000 UTC m=+4356.005744634" watchObservedRunningTime="2024-03-21 18:12:03.287744701 +0000 UTC m=+4356.006041639"
Mar 21 18:14:29 minikube kubelet[1693]: W0321 18:14:29.114524    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 18:19:29 minikube kubelet[1693]: W0321 18:19:29.116782    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 18:24:29 minikube kubelet[1693]: W0321 18:24:29.111845    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 18:29:29 minikube kubelet[1693]: W0321 18:29:29.108866    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 18:34:29 minikube kubelet[1693]: W0321 18:34:29.107643    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 18:39:29 minikube kubelet[1693]: W0321 18:39:29.107389    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 18:44:29 minikube kubelet[1693]: W0321 18:44:29.109103    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 18:47:47 minikube kubelet[1693]: I0321 18:47:47.220505    1693 topology_manager.go:215] "Topology Admit Handler" podUID="dee84d5d-55fd-415d-ab23-90723f1beb0e" podNamespace="default" podName="k8s-web-hello-f6bcdcc9c-spjw6"
Mar 21 18:47:47 minikube kubelet[1693]: I0321 18:47:47.229986    1693 topology_manager.go:215] "Topology Admit Handler" podUID="9dbec369-d147-44a9-b8c0-25751141e0b0" podNamespace="default" podName="k8s-web-hello-f6bcdcc9c-vfqqt"
Mar 21 18:47:47 minikube kubelet[1693]: I0321 18:47:47.231368    1693 topology_manager.go:215] "Topology Admit Handler" podUID="e5ebce80-78c4-4529-9741-72a9fd1be7fe" podNamespace="default" podName="k8s-web-hello-f6bcdcc9c-cbzwh"
Mar 21 18:47:47 minikube kubelet[1693]: I0321 18:47:47.302886    1693 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zgdsg\" (UniqueName: \"kubernetes.io/projected/9dbec369-d147-44a9-b8c0-25751141e0b0-kube-api-access-zgdsg\") pod \"k8s-web-hello-f6bcdcc9c-vfqqt\" (UID: \"9dbec369-d147-44a9-b8c0-25751141e0b0\") " pod="default/k8s-web-hello-f6bcdcc9c-vfqqt"
Mar 21 18:47:47 minikube kubelet[1693]: I0321 18:47:47.302986    1693 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mb8cb\" (UniqueName: \"kubernetes.io/projected/e5ebce80-78c4-4529-9741-72a9fd1be7fe-kube-api-access-mb8cb\") pod \"k8s-web-hello-f6bcdcc9c-cbzwh\" (UID: \"e5ebce80-78c4-4529-9741-72a9fd1be7fe\") " pod="default/k8s-web-hello-f6bcdcc9c-cbzwh"
Mar 21 18:47:47 minikube kubelet[1693]: I0321 18:47:47.303061    1693 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bd27t\" (UniqueName: \"kubernetes.io/projected/dee84d5d-55fd-415d-ab23-90723f1beb0e-kube-api-access-bd27t\") pod \"k8s-web-hello-f6bcdcc9c-spjw6\" (UID: \"dee84d5d-55fd-415d-ab23-90723f1beb0e\") " pod="default/k8s-web-hello-f6bcdcc9c-spjw6"
Mar 21 18:47:47 minikube kubelet[1693]: I0321 18:47:47.304759    1693 topology_manager.go:215] "Topology Admit Handler" podUID="b53cc9cb-65bd-436d-ac7a-b66682108166" podNamespace="default" podName="k8s-web-hello-f6bcdcc9c-c6b9t"
Mar 21 18:47:47 minikube kubelet[1693]: I0321 18:47:47.505070    1693 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ljq2j\" (UniqueName: \"kubernetes.io/projected/b53cc9cb-65bd-436d-ac7a-b66682108166-kube-api-access-ljq2j\") pod \"k8s-web-hello-f6bcdcc9c-c6b9t\" (UID: \"b53cc9cb-65bd-436d-ac7a-b66682108166\") " pod="default/k8s-web-hello-f6bcdcc9c-c6b9t"
Mar 21 18:47:48 minikube kubelet[1693]: I0321 18:47:48.314833    1693 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2a89f2a86b26fc8bba27a4a7c8519ddd092f549fe7ce32ff8e9706cbc95a1e2e"
Mar 21 18:47:48 minikube kubelet[1693]: I0321 18:47:48.319926    1693 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f7a77870323e8b9e864adb84796b8e7832c5a6cbc85f0f45b86c0942fae3838f"
Mar 21 18:47:48 minikube kubelet[1693]: I0321 18:47:48.334948    1693 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="fd6c20cfc9d69964c179d0c2041a94f9f5e6a890cc8d541db282ad6f8481d27f"
Mar 21 18:47:49 minikube kubelet[1693]: I0321 18:47:49.446966    1693 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/k8s-web-hello-f6bcdcc9c-spjw6" podStartSLOduration=1.5076404490000002 podCreationTimestamp="2024-03-21 18:47:47 +0000 UTC" firstStartedPulling="2024-03-21 18:47:48.11474087 +0000 UTC m=+6500.841737670" lastFinishedPulling="2024-03-21 18:47:49.053958536 +0000 UTC m=+6501.780955336" observedRunningTime="2024-03-21 18:47:49.446436515 +0000 UTC m=+6502.173433315" watchObservedRunningTime="2024-03-21 18:47:49.446858115 +0000 UTC m=+6502.173855015"
Mar 21 18:47:51 minikube kubelet[1693]: I0321 18:47:51.754423    1693 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/k8s-web-hello-f6bcdcc9c-c6b9t" podStartSLOduration=3.4110902100000002 podCreationTimestamp="2024-03-21 18:47:47 +0000 UTC" firstStartedPulling="2024-03-21 18:47:48.409392579 +0000 UTC m=+6501.136389479" lastFinishedPulling="2024-03-21 18:47:49.752678812 +0000 UTC m=+6502.479675712" observedRunningTime="2024-03-21 18:47:50.811319779 +0000 UTC m=+6503.538316579" watchObservedRunningTime="2024-03-21 18:47:51.754376443 +0000 UTC m=+6504.481373243"
Mar 21 18:47:51 minikube kubelet[1693]: I0321 18:47:51.754635    1693 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/k8s-web-hello-f6bcdcc9c-cbzwh" podStartSLOduration=2.097998284 podCreationTimestamp="2024-03-21 18:47:47 +0000 UTC" firstStartedPulling="2024-03-21 18:47:48.418564986 +0000 UTC m=+6501.145561786" lastFinishedPulling="2024-03-21 18:47:51.075169545 +0000 UTC m=+6503.802166345" observedRunningTime="2024-03-21 18:47:51.754133942 +0000 UTC m=+6504.481130742" watchObservedRunningTime="2024-03-21 18:47:51.754602843 +0000 UTC m=+6504.481599643"
Mar 21 18:47:51 minikube kubelet[1693]: I0321 18:47:51.763391    1693 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/k8s-web-hello-f6bcdcc9c-vfqqt" podStartSLOduration=2.844542002 podCreationTimestamp="2024-03-21 18:47:47 +0000 UTC" firstStartedPulling="2024-03-21 18:47:48.416621084 +0000 UTC m=+6501.143617884" lastFinishedPulling="2024-03-21 18:47:50.33542474 +0000 UTC m=+6503.062421540" observedRunningTime="2024-03-21 18:47:51.763078958 +0000 UTC m=+6504.490075758" watchObservedRunningTime="2024-03-21 18:47:51.763345658 +0000 UTC m=+6504.490342458"
Mar 21 18:49:29 minikube kubelet[1693]: W0321 18:49:29.106309    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 18:54:29 minikube kubelet[1693]: W0321 18:54:29.107456    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 19:09:44 minikube kubelet[1693]: W0321 19:09:44.818771    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 19:16:46 minikube kubelet[1693]: W0321 19:16:46.366678    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 19:22:44 minikube kubelet[1693]: W0321 19:22:44.224919    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 19:27:44 minikube kubelet[1693]: W0321 19:27:44.224941    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 19:32:44 minikube kubelet[1693]: W0321 19:32:44.222569    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 19:37:44 minikube kubelet[1693]: W0321 19:37:44.220617    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 19:42:44 minikube kubelet[1693]: W0321 19:42:44.221463    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 19:47:44 minikube kubelet[1693]: W0321 19:47:44.221175    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 19:52:44 minikube kubelet[1693]: W0321 19:52:44.219549    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 19:57:44 minikube kubelet[1693]: W0321 19:57:44.220672    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 20:02:44 minikube kubelet[1693]: W0321 20:02:44.218081    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 20:07:44 minikube kubelet[1693]: W0321 20:07:44.218186    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 20:12:44 minikube kubelet[1693]: W0321 20:12:44.217271    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 20:17:44 minikube kubelet[1693]: W0321 20:17:44.216384    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 20:22:44 minikube kubelet[1693]: W0321 20:22:44.215276    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 21 20:27:44 minikube kubelet[1693]: W0321 20:27:44.213170    1693 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> kubernetes-dashboard [bcda0851c513] <==
* 2024/03/21 17:34:43 [2024-03-21T17:34:43Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:43 [2024-03-21T17:34:43Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/03/21 17:34:43 Getting list of all replica sets in the cluster
2024/03/21 17:34:43 [2024-03-21T17:34:43Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/03/21 17:34:43 Getting list of all replication controllers in the cluster
2024/03/21 17:34:43 received 0 resources from sidecar instead of 4
2024/03/21 17:34:43 [2024-03-21T17:34:43Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/03/21 17:34:43 Getting list of all pet sets in the cluster
2024/03/21 17:34:43 received 0 resources from sidecar instead of 4
2024/03/21 17:34:43 Getting pod metrics
2024/03/21 17:34:43 [2024-03-21T17:34:43Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:43 received 0 resources from sidecar instead of 4
2024/03/21 17:34:43 received 0 resources from sidecar instead of 4
2024/03/21 17:34:43 received 0 resources from sidecar instead of 4
2024/03/21 17:34:43 received 0 resources from sidecar instead of 4
2024/03/21 17:34:43 [2024-03-21T17:34:43Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:43 Skipping metric because of error: Metric label not set.
2024/03/21 17:34:43 Skipping metric because of error: Metric label not set.
2024/03/21 17:34:43 Skipping metric because of error: Metric label not set.
2024/03/21 17:34:43 Skipping metric because of error: Metric label not set.
2024/03/21 17:34:43 Skipping metric because of error: Metric label not set.
2024/03/21 17:34:43 Skipping metric because of error: Metric label not set.
2024/03/21 17:34:43 Skipping metric because of error: Metric label not set.
2024/03/21 17:34:43 Skipping metric because of error: Metric label not set.
2024/03/21 17:34:43 [2024-03-21T17:34:43Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:43 [2024-03-21T17:34:43Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:46 [2024-03-21T17:34:46Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/03/21 17:34:46 Getting list of namespaces
2024/03/21 17:34:46 [2024-03-21T17:34:46Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:47 [2024-03-21T17:34:47Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2024/03/21 17:34:47 [2024-03-21T17:34:47Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:48 [2024-03-21T17:34:48Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/k8s-web-hello-deploy/newreplicaset request from 127.0.0.1: 
2024/03/21 17:34:48 [2024-03-21T17:34:48Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/k8s-web-hello-deploy request from 127.0.0.1: 
2024/03/21 17:34:48 Getting details of k8s-web-hello-deploy deployment in default namespace
2024/03/21 17:34:48 [2024-03-21T17:34:48Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/k8s-web-hello-deploy/oldreplicaset?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/03/21 17:34:48 Internal error occurred: No metric client provided. Skipping metrics.
2024/03/21 17:34:48 [2024-03-21T17:34:48Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:48 [2024-03-21T17:34:48Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/k8s-web-hello-deploy/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2024/03/21 17:34:48 [2024-03-21T17:34:48Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/k8s-web-hello-deploy/horizontalpodautoscaler?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/03/21 17:34:48 [2024-03-21T17:34:48Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:48 Internal error occurred: No metric client provided. Skipping metrics.
2024/03/21 17:34:48 [2024-03-21T17:34:48Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:48 [2024-03-21T17:34:48Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:48 [2024-03-21T17:34:48Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:51 [2024-03-21T17:34:51Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/k8s-web-hello-deploy request from 127.0.0.1: 
2024/03/21 17:34:51 Getting details of k8s-web-hello-deploy deployment in default namespace
2024/03/21 17:34:51 [2024-03-21T17:34:51Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2024/03/21 17:34:51 Getting list of namespaces
2024/03/21 17:34:51 [2024-03-21T17:34:51Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/k8s-web-hello-deploy/oldreplicaset?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/03/21 17:34:51 [2024-03-21T17:34:51Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/k8s-web-hello-deploy/newreplicaset request from 127.0.0.1: 
2024/03/21 17:34:51 [2024-03-21T17:34:51Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:51 [2024-03-21T17:34:51Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/k8s-web-hello-deploy/horizontalpodautoscaler?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2024/03/21 17:34:51 [2024-03-21T17:34:51Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:51 [2024-03-21T17:34:51Z] Incoming HTTP/1.1 GET /api/v1/deployment/default/k8s-web-hello-deploy/event?itemsPerPage=10&page=1&sortBy=d,lastSeen request from 127.0.0.1: 
2024/03/21 17:34:51 [2024-03-21T17:34:51Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:51 Internal error occurred: No metric client provided. Skipping metrics.
2024/03/21 17:34:51 [2024-03-21T17:34:51Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:51 [2024-03-21T17:34:51Z] Outcoming response to 127.0.0.1 with 200 status code
2024/03/21 17:34:51 Internal error occurred: No metric client provided. Skipping metrics.
2024/03/21 17:34:51 [2024-03-21T17:34:51Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> storage-provisioner [0223ebd91716] <==
* I0321 16:59:52.261812       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0321 17:00:02.361643       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

* 
* ==> storage-provisioner [9fb101ca750c] <==
* I0321 17:00:15.961431       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0321 17:00:15.977640       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0321 17:00:15.978352       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0321 17:00:33.450061       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0321 17:00:33.450368       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_f8ce6f3a-8004-4ae1-a84b-a81101e42dac!
I0321 17:00:33.450368       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"d60fd507-935a-4148-ae06-0374a19b247d", APIVersion:"v1", ResourceVersion:"25104", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_f8ce6f3a-8004-4ae1-a84b-a81101e42dac became leader
I0321 17:00:33.551628       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_f8ce6f3a-8004-4ae1-a84b-a81101e42dac!

